{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb4773c-1f20-4f8a-a655-1f99f5e9ee2b",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32bc2bd7-cbab-4ed7-b205-c98b094ea4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "#%cd /content/drive/MyDrive/MscThesis\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import gwpy\n",
    "import nds2 # conda config --add channels conda-forge \\n conda install python-nds2-client\n",
    "import numba\n",
    "\n",
    "from gwpy.table import EventTable\n",
    "from gwpy.table import GravitySpyTable\n",
    "from gwpy.detector import ChannelList, Channel\n",
    "from gwpy.time import tconvert\n",
    "from gwpy.timeseries import TimeSeries #if this does not work, remove h5py and reinstall h5py\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.table import vstack\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import DatasetFolder, ImageFolder\n",
    "from tqdm import tqdm\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, \\\n",
    "accuracy_score\n",
    "from avalanche.models import SlimResNet18, MTSlimResNet18, SimpleCNN\n",
    "from avalanche.models import as_multitask, IncrementalClassifier\n",
    "from avalanche.models.utils import avalanche_model_adaptation\n",
    "from avalanche.training import Naive, LwF, GenerativeReplay, ICaRLLossPlugin, ICaRL, EWC, AR1, LFL, AGEM\n",
    "from avalanche.logging import (\n",
    "    InteractiveLogger,\n",
    "    TextLogger,\n",
    "    CSVLogger,\n",
    ")\n",
    "from avalanche.training.plugins import EvaluationPlugin, ReplayPlugin, LwFPlugin, EarlyStoppingPlugin, AGEMPlugin\n",
    "from avalanche.training.plugins.lr_scheduling import LRSchedulerPlugin\n",
    "from avalanche.benchmarks import nc_benchmark, ni_benchmark\n",
    "from avalanche.benchmarks.utils import AvalancheDataset\n",
    "from avalanche.benchmarks.scenarios.dataset_scenario import benchmark_from_datasets\n",
    "from avalanche.benchmarks.scenarios.supervised import class_incremental_benchmark\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics,loss_metrics, \\\n",
    "timing_metrics, cpu_usage_metrics, StreamConfusionMatrix, disk_usage_metrics, gpu_usage_metrics, \\\n",
    "confusion_matrix_metrics, bwt_metrics, forward_transfer_metrics, ram_usage_metrics, images_samples_metrics\n",
    "from avalanche.evaluation.metrics import Accuracy, BWT, Forgetting, ForwardTransfer\n",
    "import multiprocessing as mp\n",
    "\n",
    "import my_utils\n",
    "import my_architectures\n",
    "import my_dataloaders\n",
    "import my_gwpy_and_fractals\n",
    "import my_transformations\n",
    "\n",
    "IMG_SIZE = (224,224)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# calculating the amount of workers usable\n",
    "number_of_workers = mp.cpu_count()\n",
    "number_of_workers = int(number_of_workers/2)\n",
    "print(number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74e6053a-9941-471c-b88d-5aabbcb8b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = %pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c2be1bc-93e4-492f-a98a-c98d02d7b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the correct directories\n",
    "#train_dir = 'D:\\\\Users\\\\Brian\\\\TrainingSet_CL_1\\\\TrainingSet_CL_1'\n",
    "train_dir = 'C:\\\\Users\\\\Brian.Baert\\\\TrainingSet_CL_1'\n",
    "#train_dir_2 = 'D:\\\\Users\\Brian\\\\TrainingSet_CL_2\\\\TrainingSet_CL_2'\n",
    "train_dir_2 = 'C:\\\\Users\\\\Brian.Baert\\\\TrainingSet_CL_2'\n",
    "#val_dir = 'D:\\\\Users\\\\Brian\\\\TestSet_CL\\\\TestSet_CL'\n",
    "val_dir = 'C:\\\\Users\\\\Brian.Baert\\\\ValidationSet_CL'\n",
    "#test_dir = 'D:\\\\Users\\\\Brian\\\\TestSet_CL\\\\TestSet_CL'\n",
    "test_dir = 'C:\\\\Users\\\\Brian.Baert\\\\TestSet_CL'\n",
    "\n",
    "# Read in class labels\n",
    "class_file = open(\"classes.txt\", \"r\")\n",
    "classes = class_file.read()\n",
    "classes = classes.split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd9ef23f-32d2-4689-8cd8-19b3e0a55eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract glitches and IFO ---> ONLY ONCE\n",
    "L1_filename = \"L1_glitches.txt\"\n",
    "H1_filename = \"H1_glitches.txt\"\n",
    "\n",
    "glitch_names_L1 = set()\n",
    "glitch_names_H1 = set()\n",
    "\n",
    "for root, _, files in os.walk(train_dir):\n",
    "    for filename in files:\n",
    "        if filename.startswith(\"H1_\"):\n",
    "            parts = filename.split(\"_\")\n",
    "            if len(parts) >= 3:\n",
    "                glitch_names_H1.add(parts[1])\n",
    "        if filename.startswith(\"L1_\"):\n",
    "            parts = filename.split(\"_\")\n",
    "            if len(parts) >=2:\n",
    "                glitch_names_L1.add(parts[1])\n",
    "\n",
    "with open(os.path.join(current_dir, L1_filename), \"w\") as output_file:\n",
    "    for name in glitch_names_L1:\n",
    "        output_file.write(f\"{name}\\n\")\n",
    "        \n",
    "with open(os.path.join(current_dir, H1_filename), \"w\") as output_file:\n",
    "    for name in glitch_names_H1:\n",
    "        output_file.write(f\"{name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9195345-eec3-40d2-86b7-eb0299a1be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "with open(\"H1_glitches.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        filenames.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5f621b-73a4-43b9-9536-9ac9736ca8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment variables\n",
    "os.environ[\"GRAVITYSPY_DATABASE_USER\"] = \"mla\"\n",
    "os.environ[\"GRAVITYSPY_DATABASE_PASSWD\"] = \"gl1tch35Rb4d!\"\n",
    "glitch_labels = ['1400Ripples', '1080Lines', 'Air_Compressor', \n",
    "                 'Blip', 'Chirp', 'Extremely_Loud', 'Helix', 'Koi_Fish', 'Light_Modulation', \n",
    "                 'Low_Frequency_Burst', 'Low_Frequency_Lines', 'No_Glitch', 'None_of_the_Above', \n",
    "                 'Paired_Doves', 'Power_Line', 'Repeating_Blips', 'Scattered_Light', 'Scratchy', \n",
    "                 'Tomte', 'Violin_Mode', 'Wandering_Line', 'Whistle']\n",
    "O3b_start = 1256655618\n",
    "O3b_stop = 1269363618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b66206f-dac8-47ce-923a-1edd703baba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "O3b_csv = pd.read_csv('Data/data_o3b_high_confidence.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "601e772a-203b-4a49-b848-08756a92d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "glitch_id = '6qQpdFEYLg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9f1e904-c45c-481b-b0fc-715679c37858",
   "metadata": {},
   "outputs": [],
   "source": [
    "glitch_start_time = int(O3b_csv[O3b_csv.id == glitch_id]['GPStime'])-2\n",
    "glitch_stop_time = int(O3b_csv[O3b_csv.id == glitch_id]['GPStime'])+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcd1aad0-94ed-42f2-ad36-67ce3eb79f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0.25\n",
    "sampling_rate = 16384\n",
    "#sampling_rate = 512\n",
    "decimate = 64\n",
    "#channel = 'L1:DCS-CALIB_STRAIN_C01_AR'\n",
    "#hannel = 'H1:DCS-CALIB_STRAIN_C01_AR'\n",
    "channel = 'L1:PEM-EY_MIC_VEA_PLUSY_DQ'\n",
    "#channel = 'H1:LSC-PRCL_OUT_DQ' --> not available\n",
    "#channel = 'H1:LSC-PRCL_IN1_DQ' --> not available\n",
    "#channel = 'L1:LSC-PRCL_IN1_DQ'\n",
    "#channel = 'L1:LSC-PRCL_OUT_DQ'\n",
    "#channel = 'L1:ASC-CHARD_Y_OUT_DQ'\n",
    "#server = 'losc-nds.ligo.org'\n",
    "server = 'nds.gwosc.org'\n",
    "alpha = 1\n",
    "data_end = sampling_rate * (glitch_stop_time - glitch_start_time)\n",
    "data = pd.DataFrame(columns=['time', 'value'])\n",
    "data_length = int((glitch_stop_time - glitch_start_time) / step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42d08a2d-f149-404b-a5e5-7e0394ded954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the TimeSeries data\n",
      "Data is not available\n",
      "fetch_and_whiten_data: 3735.234498977661 ms\n"
     ]
    }
   ],
   "source": [
    "test = int(O3b_csv[O3b_csv.id == glitch_id]['GPStime'])\n",
    "testdata = my_gwpy_and_fractals.fetch_and_whiten_data(glitch_start_time, glitch_stop_time, server, sampling_rate, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a84b7ba-e4f7-45ce-983c-3baad2489900",
   "metadata": {},
   "outputs": [],
   "source": [
    "chanList = ChannelList.query_nds2('*', host=server)\n",
    "print(\"Found {0} channels\\n\".format(len(chanList)))\n",
    "print(\"Printing first 10 channels ...\")\n",
    "auxlist = [chan for chan in chanList if 'H1:LSC' in chan.name]\n",
    "print(\"Here are channels containing L1 in the name ...\")\n",
    "for chan in auxlist:\n",
    "    print(chan.name, chan.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb4d75a-7f2d-4c0c-b34f-90c20fa77d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_conditioned = my_gwpy_and_fractals.fetch_and_whiten_data(glitch_start_time, glitch_stop_time, server, sampling_rate, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f5308-8b6c-44c7-b223-61f813f64eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_conditioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c39651-1bc0-4185-8607-a58b6654b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stamps = np.arange(0, int(data_end/2))\n",
    "time_stamps_array = np.split(time_stamps, data_length)\n",
    "for element in time_stamps_array:\n",
    "    try:\n",
    "        data = pd.concat([data, pd.DataFrame(pd.Series([element, data_conditioned[element]], index=['time', 'value'])).T], ignore_index=True)\n",
    "    except:\n",
    "        print('No data or exception')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa6a98a-792a-44f3-b1b0-982ff7033340",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_conditioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb9892-fb9f-4201-87cf-810d4649f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = data_length\n",
    "data_fd = pd.DataFrame(columns=['time', 'fd'])\n",
    "\n",
    "for chunk in range(0, number_of_chunks):\n",
    "    start_chunktime = chunk * step + glitch_start_time\n",
    "    check_valid = data.iloc[chunk].value\n",
    "    if np.isnan(check_valid.all()):\n",
    "        print(f\"Warning, undefined data, FD is set to zero at time {start_chunktime}.\")\n",
    "        continue\n",
    "    print(f\"Computing the var estimator for data starting at {start_chunktime} time.\")\n",
    "    #print(data.iloc[chunk])\n",
    "    est_eval = my_gwpy_and_fractals.var_function(data.iloc[chunk], decimate)\n",
    "    est_fit = my_gwpy_and_fractals.fit_est(est_eval, sampling_rate)\n",
    "    fractal_dimension = 2.-est_fit[3]\n",
    "    print(f\"fd= {fractal_dimension}\")\n",
    "    data_fd = pd.concat([data_fd, pd.DataFrame([[start_chunktime, fractal_dimension]], columns=['time', 'fd'])], ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383cf22a-d855-40c5-b3e0-dc5550ae6f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"FD_data/fd_calculation_\" + glitch_id + \"_aux_channel_\" + channel.replace(':', '_') + \".txt\"\n",
    "data_fd.to_csv(filename, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc3f766-f9f7-4126-b7c9-4ef4ed5e1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_min_y = 1.0\n",
    "plot_max_y = 2.0\n",
    "\n",
    "ax.set_title(channel, fontsize=11)\n",
    "ax.tick_params(direction='in', length=6, width=0.5, colors='black', grid_color='r', grid_alpha=1)\n",
    "ax.set_xlabel(f\"{step} Seconds from GPS time {glitch_start_time}\", fontsize=11)\n",
    "ax.set_ylabel(\"Fractal dimension\",fontsize=11)\n",
    "ax.set_xlim([glitch_start_time,glitch_stop_time])\n",
    "ax.set_ylim([plot_min_y, plot_max_y])\n",
    "ax.set_xticks(np.linspace(glitch_start_time, glitch_stop_time, 9))\n",
    "ax.set_xticklabels(np.linspace(-2, 2, 9))\n",
    "\n",
    "ax.grid(which='both', axis='both', linestyle='--', color='k', linewidth=0.5)\n",
    "\n",
    "time = data_fd['time'].values\n",
    "step = time[1]-time[0]\n",
    "fd = data_fd['fd'].values\n",
    "ax.errorbar(time+step/2, fd, xerr=step/2, fmt='.', c='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc31904-9bda-4228-9cac-dd53fb1158b2",
   "metadata": {},
   "source": [
    "## Calculate FD for all glitches in train_dir (H1 and L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ab4ff80-f1bf-4037-8eae-40e9a02a2c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "with open(\"H1_glitches.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        filenames.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f3d2ad3-a7a7-4b76-be6d-3f1f855c5271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_interval(number, interval_dataframe):\n",
    "  \"\"\"\n",
    "  This function checks if a number falls within any of the intervals in a DataFrame.\n",
    "\n",
    "  Args:\n",
    "      number: The number to check.\n",
    "      interval_dataframe: A DataFrame with two columns, 'start' and 'stop', representing the intervals.\n",
    "\n",
    "  Returns:\n",
    "      A boolean value indicating whether the number falls within any interval (True) or not (False).\n",
    "  \"\"\"\n",
    "  return (interval_dataframe[0] <= number) & (number <= interval_dataframe[1]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66dd80c1-bf15-40e5-807a-bbf1526e2383",
   "metadata": {},
   "outputs": [],
   "source": [
    "glitch_segments_H1 = []\n",
    "gsH1 = pd.read_csv(\"Data/H1-SEGS-AUX_CLN.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d057b76-abb7-46d7-a642-cdf1384735bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with the glitch labeled vwBCf8PO2M\n",
      "FD calculation for iKvcI8ZZaE has started.\n",
      "Getting the TimeSeries data\n",
      "Data is not available\n",
      "fetch_and_whiten_data: 3617.1038150787354 ms\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "The value of data_conditioned is: None\n",
      "No fd calculations made because no data available\n",
      "Error with the glitch labeled LgnxYwPSLE\n",
      "FD calculation for keFmTMcib6 has started.\n",
      "Getting the TimeSeries data\n",
      "fetch_and_whiten_data: 4135.688304901123 ms\n",
      "The value of data_conditioned is: [-0.78131657 -0.45445841  1.43609986 ...  1.08917747 -1.285113\n",
      "  1.20618777]\n",
      "The value of data_length is: 16\n",
      "FD calculation for Ucue07t7cU has started.\n",
      "Getting the TimeSeries data\n",
      "fetch_and_whiten_data: 3827.784776687622 ms\n",
      "The value of data_conditioned is: [-0.36469543 -0.02003004 -0.22031567 ... -2.29517428  0.50602682\n",
      " -0.52969826]\n",
      "The value of data_length is: 16\n",
      "Error with the glitch labeled Ek91Yi5eGn\n",
      "FD calculation for 1jkABzWkSW has started.\n",
      "Getting the TimeSeries data\n",
      "fetch_and_whiten_data: 4079.589366912842 ms\n",
      "The value of data_conditioned is: [-0.76457711  0.02138203  0.29090363 ...  0.31118231 -1.30864713\n",
      " -0.25355618]\n",
      "The value of data_length is: 16\n",
      "Error with the glitch labeled e9o5uWysa4\n",
      "FD calculation for OIblbbd8oQ has started.\n",
      "Getting the TimeSeries data\n",
      "fetch_and_whiten_data: 3821.1450576782227 ms\n",
      "The value of data_conditioned is: [ 1.16385782  0.52701776  0.37330858 ... -0.82162813  2.05902201\n",
      " -1.12990322]\n",
      "The value of data_length is: 16\n",
      "FD calculation for npRrSfu1gQ has started.\n",
      "Getting the TimeSeries data\n",
      "Data is not available\n",
      "fetch_and_whiten_data: 3082.9238891601562 ms\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "The value of data_conditioned is: None\n",
      "No fd calculations made because no data available\n",
      "FD calculation for T5GFvpCNBB has started.\n",
      "Getting the TimeSeries data\n",
      "Data is not available\n",
      "fetch_and_whiten_data: 3142.638921737671 ms\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "The value of data_conditioned is: None\n",
      "No fd calculations made because no data available\n",
      "FD calculation for k0k6bAHxnm has started.\n",
      "Getting the TimeSeries data\n",
      "Data is not available\n",
      "fetch_and_whiten_data: 3117.0332431793213 ms\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "No data or exception\n",
      "The value of data_conditioned is: None\n",
      "No fd calculations made because no data available\n",
      "Error with the glitch labeled Su57FpvYxN\n",
      "FD calculation for ktHXeaWDpj has started.\n",
      "Getting the TimeSeries data\n",
      "fetch_and_whiten_data: 4761.739492416382 ms\n",
      "The value of data_conditioned is: [ 1.0381582  -0.79090859  0.47179758 ...  0.16114901 -0.73054641\n",
      "  2.19542189]\n",
      "The value of data_length is: 16\n",
      "FD calculation for pBlA7jSZBM has started.\n",
      "Getting the TimeSeries data\n",
      "fetch_and_whiten_data: 5457.858324050903 ms\n",
      "The value of data_conditioned is: [ 0.82366292 -1.01628578  1.0852462  ... -0.07624108 -1.00100904\n",
      "  1.54607573]\n",
      "The value of data_length is: 16\n",
      "Error with the glitch labeled h79UFCfoVS\n",
      "FD calculation for 6Cake4dPJN has started.\n",
      "Getting the TimeSeries data\n",
      "fetch_and_whiten_data: 5412.851333618164 ms\n",
      "The value of data_conditioned is: [ 2.39305127  0.18328533  0.24673368 ... -0.23190428 -0.81455323\n",
      " -1.11439933]\n",
      "The value of data_length is: 16\n",
      "FD calculation for kMXVuRdq5y has started.\n",
      "Getting the TimeSeries data\n",
      "fetch_and_whiten_data: 5965.467929840088 ms\n",
      "The value of data_conditioned is: [ 1.90714124 -0.85480129 -0.9826996  ... -0.51662324 -0.23571019\n",
      " -0.94498016]\n",
      "The value of data_length is: 16\n",
      "FD calculation for 3UbzGrtuVY has started.\n",
      "Getting the TimeSeries data\n",
      "fetch_and_whiten_data: 5968.149423599243 ms\n",
      "The value of data_conditioned is: [-0.10564148 -0.41484723  0.21818489 ...  1.23906769 -1.8470109\n",
      "  1.11052565]\n",
      "The value of data_length is: 16\n",
      "FD calculation for SJkDr0OLtm has started.\n",
      "Getting the TimeSeries data\n",
      "fetch_and_whiten_data: 4716.864824295044 ms\n",
      "The value of data_conditioned is: [-1.48340632  0.12786987  0.26722163 ... -0.28201092 -0.76663412\n",
      "  0.15182145]\n",
      "The value of data_length is: 16\n",
      "FD calculation for 0ZqOd86QtD has started.\n",
      "Getting the TimeSeries data\n",
      "fetch_and_whiten_data: 4852.081298828125 ms\n",
      "The value of data_conditioned is: [-0.16310961 -0.69779437  2.29420642 ...  0.27081209 -1.87717081\n",
      "  1.35283431]\n",
      "The value of data_length is: 16\n",
      "FD calculation for fmC6An28ah has started.\n",
      "Getting the TimeSeries data\n",
      "fetch_and_whiten_data: 5324.800491333008 ms\n",
      "The value of data_conditioned is: [ 1.02324527 -1.07432086  0.91589489 ... -0.95003676  1.07555236\n",
      " -0.51432504]\n",
      "The value of data_length is: 16\n",
      "FD calculation for vyqxCJOM8P has started.\n",
      "Getting the TimeSeries data\n"
     ]
    }
   ],
   "source": [
    "for fn in filenames:\n",
    "    glitch_id = str(fn)\n",
    "    peak_time = O3b_csv[O3b_csv.id == glitch_id]['GPStime']\n",
    "    if not peak_time.empty:\n",
    "        start = int(peak_time)-2\n",
    "        tmp1 = is_in_interval(start, gsH1)\n",
    "        if tmp1.any():\n",
    "            stop = int(peak_time)+2\n",
    "            tmp2 = is_in_interval(stop, gsH1)\n",
    "            if tmp2.any():\n",
    "                print(f\"FD calculation for {glitch_id} has started.\")\n",
    "                data_fd = my_gwpy_and_fractals.calculate_fd_files(start, stop, server, sampling_rate, channel, step, decimate, alpha, False)\n",
    "                if data_fd is not None:\n",
    "                    filename = \"FD_data/fd_calculation_\" + glitch_id + \"_aux_channel_\" + channel.replace(':', '_') + \".txt\"\n",
    "                    data_fd.to_csv(filename, sep='\\t', index=False)\n",
    "                else:\n",
    "                    print('No fd calculations made because no data available')\n",
    "            else:\n",
    "                print(f\"{stop} does not fall within any of the intervals\")\n",
    "        else:\n",
    "          print(f\"{start} does not fall within any of the intervals\")\n",
    "    else:\n",
    "        print(f\"Error with the glitch labeled {glitch_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa90e69-bf1e-48e4-9113-1c79b27193b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
