{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/brianbaert/MscThesis/blob/main/MscThesis_Playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOAiIcv_lMu8"
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t1jbz0_g5bZW",
    "outputId": "68b123d7-8ec2-4e8c-d4da-9c025017c0f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%cd /content/drive/MyDrive/MscThesis\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rVEI5x8RadVA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import DatasetFolder, ImageFolder\n",
    "from tqdm import tqdm\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, \\\n",
    "accuracy_score\n",
    "from avalanche.models import SlimResNet18, MTSlimResNet18, SimpleCNN\n",
    "from avalanche.models import as_multitask, IncrementalClassifier\n",
    "from avalanche.models.utils import avalanche_model_adaptation\n",
    "from avalanche.training import Naive, LwF, GenerativeReplay, ICaRLLossPlugin, ICaRL, EWC, AR1, LFL, AGEM\n",
    "from avalanche.logging import (\n",
    "    InteractiveLogger,\n",
    "    TextLogger,\n",
    "    CSVLogger,\n",
    ")\n",
    "from avalanche.training.plugins import EvaluationPlugin, ReplayPlugin, LwFPlugin, EarlyStoppingPlugin, AGEMPlugin\n",
    "from avalanche.training.plugins.lr_scheduling import LRSchedulerPlugin\n",
    "from avalanche.benchmarks import nc_benchmark, ni_benchmark\n",
    "from avalanche.benchmarks.utils import AvalancheDataset\n",
    "from avalanche.benchmarks.scenarios.dataset_scenario import benchmark_from_datasets\n",
    "from avalanche.benchmarks.scenarios.supervised import class_incremental_benchmark\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics,loss_metrics, \\\n",
    "timing_metrics, cpu_usage_metrics, StreamConfusionMatrix, disk_usage_metrics, gpu_usage_metrics, \\\n",
    "confusion_matrix_metrics, bwt_metrics, forward_transfer_metrics, ram_usage_metrics, images_samples_metrics\n",
    "from avalanche.evaluation.metrics import Accuracy, BWT, Forgetting, ForwardTransfer\n",
    "import multiprocessing as mp\n",
    "\n",
    "import my_utils\n",
    "import my_architectures\n",
    "import my_dataloaders\n",
    "import my_gwpy_and_fractals\n",
    "import my_transformations\n",
    "\n",
    "IMG_SIZE = (224,224)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# calculating the amount of workers usable\n",
    "number_of_workers = mp.cpu_count()\n",
    "number_of_workers = int(number_of_workers/2)\n",
    "print(number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Wpa8snbd2P8Q"
   },
   "outputs": [],
   "source": [
    "# Setting the correct directories\n",
    "train_dir = 'C:\\\\Users\\\\Brian.Baert\\\\TrainingSet_CL_1'\n",
    "train_dir_2 = 'C:\\\\Users\\Brian.Baert\\\\TrainingSet_CL_2'\n",
    "val_dir = 'C:\\\\Users\\\\Brian.Baert\\\\ValidationSet_CL'\n",
    "test_dir = 'C:\\\\Users\\\\Brian.Baert\\\\TestSet_CL'\n",
    "\n",
    "# Read in class labels\n",
    "class_file = open(\"classes.txt\", \"r\")\n",
    "classes = class_file.read()\n",
    "classes = classes.split(\", \")\n",
    "\n",
    "# Read in inverse weights used to cope with the imbalanced dataset\n",
    "#class_weights_file = open(\"class_weights.txt\", \"r\")\n",
    "#temp = class_weights_file.read()\n",
    "#temp_split = temp.split(\", \")\n",
    "#class_weights =  [float(weight) for weight in temp_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jz23eVLldHhF",
    "outputId": "b43f99a3-4c6f-4eb2-d62a-8608422b899f"
   },
   "outputs": [],
   "source": [
    "meta_train_v1 = pd.read_csv('C:\\\\Users\\\\Brian.Baert\\\\GravitySpy\\\\trainingset_v1d1_metadata.csv')\n",
    "print(meta_train_v1[meta_train_v1['sample_type']=='train']['label'].value_counts())\n",
    "classes = meta_train_v1['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CW-cEM0DJMom",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# DL Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5luuVVS8cUE"
   },
   "outputs": [],
   "source": [
    "train_set = my_dataloaders.GravitySpy_1_0_dataset(root=train_dir, cls=classes, transform=my_transformations.transformGray)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "val_set = my_dataloaders.GravitySpy_1_0_dataset(root=val_dir, cls=classes, transform=my_transformations.transformGray)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=True, num_workers=number_of_workers)\n",
    "test_set = my_dataloaders.GravitySpy_1_0_dataset(root=test_dir, cls=classes, transform=my_transformations.transformGray)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOyw0KMTCZQ2"
   },
   "outputs": [],
   "source": [
    "train_set = my_dataloaders.GravitySpy_0_5_dataset(root=train_dir, cls=classes, transform=my_transformations.transformGray)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "val_set = my_dataloaders.GravitySpy_0_5_dataset(root=val_dir, cls=classes, transform=my_transformations.transformGray)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=True)\n",
    "test_set = my_dataloaders.GravitySpy_0_5_dataset(root=test_dir, cls=classes, transform=my_transformations.transformGray)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XuoIZcjFLCSk",
    "outputId": "41bcc824-052b-42a4-e8d6-acd450325da6"
   },
   "outputs": [],
   "source": [
    "print(\"The training loader contains {} instances, the val loader contains {} instances and the test loader contains {} instances\".format(len(train_loader), len(val_loader), len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgVNrbCl_OQo",
    "outputId": "fe5d2346-f865-4985-bf03-e766da533cfe"
   },
   "outputs": [],
   "source": [
    "class_counts = train_set.count_class_instances()\n",
    "class_weights = []\n",
    "for class_name, count in class_counts.items():\n",
    "  print(f\"{class_name}: {count} instances\")\n",
    "  class_weights.append(1.0/(count/len(train_set)))\n",
    "\n",
    "class_weights\n",
    "\n",
    "with open(\"class_weights.txt\", \"w\") as output:\n",
    "    output.write(str(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akFKILwW9FFq",
    "outputId": "d27e49d1-6889-40ae-f8b7-fe65dc640487"
   },
   "outputs": [],
   "source": [
    "# Create Neural Network architecture for finetuning\n",
    "myNet = my_architectures.BaselineGrayscaleNet_resnet18()\n",
    "myNet.to(device)\n",
    "print(myNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BP8GXkixkmAS"
   },
   "outputs": [],
   "source": [
    "# Pytorch Training loop\n",
    "epoch_test_loss = 0\n",
    "epoch_test_acc = 0\n",
    "epoch_test_correct = 0\n",
    "epoch_test_total = 0\n",
    "best_vloss = 1_000_000.\n",
    "avg_loss = 0\n",
    "last_loss = 0\n",
    "timestamp = my_utils.get_timestamp()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32), label_smoothing=0.2)\n",
    "optimizer = Adam(myNet.parameters(), lr=0.001, weight_decay=1e-3) #LR from the study of Tiago Fernandes\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 10\n",
    "epoch_number = 0\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "  total_correct = 0\n",
    "  total_samples = 0\n",
    "  print('EPOCH {}: '.format(epoch_number + 1))\n",
    "  train_loss = 0.0\n",
    "  myNet.train(True)\n",
    "\n",
    "  ## TRAINING ONE EPOCH\n",
    "  for i, data in enumerate(train_loader, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    # forward + backward + optimize\n",
    "    outputs = myNet(inputs)\n",
    "    # compute loss and gradient\n",
    "    loss = criterion(outputs, labels)\n",
    "    #add part of regularization\n",
    "    l2_reg = torch.tensor(0.0)\n",
    "    for param in myNet.parameters():\n",
    "      if param.requires_grad: #exclude frozen layers\n",
    "        l2_reg += torch.norm(param, p=2)\n",
    "    loss += 1e-5 * l2_reg\n",
    "    #end part of regularization\n",
    "    loss.backward()\n",
    "    # adjust learning weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    train_loss += loss.item()\n",
    "    total_correct += (predicted == labels).sum().item()\n",
    "    total_samples += labels.size(0)\n",
    "\n",
    "    if i % 16 == 15:\n",
    "      last_loss = train_loss / 16\n",
    "      print('  batch {} loss: {}, acc: {}'.format(i+1, last_loss, 100*total_correct/total_samples))\n",
    "      tb_x = epoch * len(train_loader) + i + 1\n",
    "      train_loss=0.\n",
    "    avg_loss = last_loss\n",
    "\n",
    "  ## VALIDATION\n",
    "  running_vloss = 0.0\n",
    "  myNet.eval()\n",
    "\n",
    "  # disable gradient computation for validation\n",
    "  with torch.no_grad():\n",
    "    for i, vdata in enumerate(val_loader):\n",
    "      vinputs, vlabels = vdata\n",
    "      vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
    "      voutputs = myNet(vinputs)\n",
    "      vloss = criterion(voutputs, vlabels)\n",
    "      running_vloss += vloss\n",
    "  avg_vloss = running_vloss / (i+1)\n",
    "  print('LOSS train {:.4f} valid {:.4f} after {} seconds'.format(avg_loss, avg_vloss, time.time()-start))\n",
    "\n",
    "  if avg_vloss < best_vloss:\n",
    "    best_vloss = avg_vloss\n",
    "    model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "    torch.save(myNet.state_dict(), model_path)\n",
    "\n",
    "  #my_utils.checkpoint(myNet, f\"epoch-{epoch}.pth\")\n",
    "  epoch_number += 1\n",
    "\n",
    "end = time.time()\n",
    "print('Training finished after ', end-start, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFwrV-TIlBCz",
    "outputId": "5ce8f0e2-ed19-4fc1-d36c-9708971b38d8"
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "#torch.save(myNet.state_dict(), '/content/drive/MyDrive/MscThesis/thesis_baseline_finetune_0_5.pth')\n",
    "torch.save(myNet.state_dict(), 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_baseline_finetune_0_5.pth')\n",
    "print(\"Saved Pytorch Model state to thesis_baseline.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPb3Hcs4RbsK"
   },
   "outputs": [],
   "source": [
    "#myNet.load_state_dict(torch.load('/content/drive/MyDrive/MscThesis/thesis_baseline_finetune_1_0.pth'))\n",
    "myNet.load_state_dict(torch.load('C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_baseline_finetune_1_0.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijMNYNHxBB8D",
    "outputId": "c266284b-0852-40de-b6b7-31660d998d47"
   },
   "outputs": [],
   "source": [
    "my_utils.n_test_predictions(myNet, test_loader, classes, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPqfIK6qK_mg"
   },
   "outputs": [],
   "source": [
    "y_pred_list, y_true_list = my_utils.get_predictions(myNet, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETUYsXlKM2CB"
   },
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "from sklearn.metrics import f1_score\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nt51bp1_lJa-"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "# Use seaborn heatmap for visualization\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=classes, yticklabels=classes)\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "figTemp = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "figTemp.savefig('confusion_matrix_baseline_1_0.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyT7cWcJQAhm"
   },
   "outputs": [],
   "source": [
    "print(f\"F1 Score for each class: {f1}\")\n",
    "print(f\"The average F1 score is: {f1_score(y_true_list, y_pred_list, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qt4Bt4mURPMY"
   },
   "outputs": [],
   "source": [
    "# Create a horizontal bar plot for F1 scores with different colors\n",
    "plt.figure(figsize=(10,7))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(classes)))\n",
    "plt.barh(np.arange(len(classes)), f1, color=colors, align='center', alpha=0.5)\n",
    "plt.yticks(np.arange(len(classes)), classes)\n",
    "plt.xlabel('F1 Score')\n",
    "plt.title('F1 Score for Each Class')\n",
    "figTemp = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "figTemp.savefig('f1_scores_baseline_1_0.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPZtEpHZHjCZ"
   },
   "source": [
    "# CL Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) CLASS INCREMENTAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, dataloader, benchmark and eval plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JiVzZcsjHlWV"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = my_architectures.BaselineColorNet_resnet18()\n",
    "model.to(device)\n",
    "# log to text file\n",
    "text_logger = TextLogger(open(\"naive_log.txt\", \"a\"))\n",
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()\n",
    "csv_logger = CSVLogger()\n",
    "classes = my_utils.get_classes_from_dir(train_dir_2)\n",
    "class_to_indx = my_utils.classes_to_indices(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blip', 'Blip_Low_Frequency', 'Extremely_Loud', 'Fast_Scattering', 'Koi_Fish', 'Low_Frequency_Burst', 'Low_Frequency_Lines', 'Scattered_Light', 'Tomte', 'Whistle']\n"
     ]
    }
   ],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xaqHp2gOQb0j"
   },
   "outputs": [],
   "source": [
    "train_set = my_dataloaders.GravitySpy_1_0_dataset(root=train_dir, cls=classes, transform=my_transformations.transformAV_224_Crop)\n",
    "val_set = my_dataloaders.GravitySpy_1_0_dataset(root=val_dir, cls=classes, transform=my_transformations.transformAV_224)\n",
    "test_set = my_dataloaders.GravitySpy_1_0_dataset(root=test_dir, cls=classes, transform=my_transformations.transformAV_224)\n",
    "\n",
    "train_set_av = AvalancheDataset(train_set)\n",
    "train_loader_av = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "\n",
    "val_set_av = AvalancheDataset(val_set)\n",
    "val_loader_av = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "\n",
    "test_set_av = AvalancheDataset(test_set)\n",
    "test_loader_av = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "\n",
    "train_set_av.targets = train_set.labels\n",
    "test_set_av.targets = test_set.labels\n",
    "val_set_av.targets = val_set.labels\n",
    "\n",
    "train_set_av.uniques = list(set(train_set.labels))\n",
    "test_set_av.uniques = list(set(test_set.labels))\n",
    "val_set_av.uniques = list(set(val_set.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LEhQaFGEQcTz"
   },
   "outputs": [],
   "source": [
    "# DEFINE THE BENCHMARK\n",
    "# CL custom benchmark, here we opt for the generator New Classes (NC)\n",
    "# Given a sequence of train and test datasets this creates the continual stream as a series of experiences.\n",
    "bm = nc_benchmark(\n",
    "    train_dataset=train_set_av,\n",
    "    test_dataset=val_set_av,\n",
    "    n_experiences=5,\n",
    "    shuffle=True,\n",
    "    seed=1234,\n",
    "    task_labels=False,\n",
    "    class_ids_from_zero_in_each_exp=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE EVALUATION PLUGIN\n",
    "# The evaluation plugin manages the metrics computation. It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    forgetting_metrics(experience=False, stream=True),\n",
    "    ram_usage_metrics(experience=True, stream=True, epoch=False),\n",
    "    bwt_metrics(experience=True, stream=True),\n",
    "    loggers=[interactive_logger, text_logger, csv_logger],\n",
    "    strict_checks=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_utils.plot_first_image(train_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_utils.plot_first_image(val_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_utils.plot_first_image(test_loader_av)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive strategy with cross entropy loss, adam and replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yK88QPCmQzA_",
    "outputId": "88bc7c53-2fa7-4199-fd80-553531c49ced"
   },
   "outputs": [],
   "source": [
    "# OPTIMIZER and CRITERION\n",
    "optimizer = AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = CrossEntropyLoss()\n",
    "#criterion = CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32), label_smoothing=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAIVE STRATEGY\n",
    "cl_strategy = Naive(\n",
    "    model=model, optimizer=optimizer, criterion=criterion, train_mb_size=32, train_epochs=1, \n",
    "    eval_mb_size=16, device=device, evaluator=eval_plugin, \n",
    "    plugins=[ReplayPlugin(mem_size=2*len(train_set)), EarlyStoppingPlugin(patience=2, val_stream_name='train')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment with strategy: <avalanche.training.supervised.strategy_wrappers.Naive object at 0x0000027595D229B0>\n",
      "Start of experience:  0\n",
      "Current Classes:  [4, 5]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 13/13 [01:03<00:00,  4.88s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9183\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8850\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "[[-0.11281399 -0.09572139  0.01786538 ...  0.08387174\n",
      "  -0.01636911  0.00876771]\n",
      " [ 0.06693275  0.07305225 -0.04529576 ...  0.08414421\n",
      "   0.085862   -0.08169781]\n",
      " [-0.07740983 -0.08762407 -0.03389743 ... -0.05630941\n",
      "   0.03445233 -0.09065166]\n",
      " ...\n",
      " [ 0.01689652 -0.0278723  -0.07331853 ... -0.05398433\n",
      "   0.08926272  0.03432913]\n",
      " [ 0.0252423   0.00763895 -0.04397242 ... -0.12119881\n",
      "  -0.0430718  -0.11908836]\n",
      " [ 0.08932412  0.09180878  0.00588199 ...  0.05640303\n",
      "  -0.07116015 -0.09312955]]\n",
      "4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Index(...) must be called with a collection of some kind, 4 was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TRAINING\u001b[39;00m\n\u001b[0;32m      2\u001b[0m results\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmy_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcl_simple_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcl_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_workers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\MscThesis\\my_utils.py:40\u001b[0m, in \u001b[0;36mtimeit.<locals>.timed\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtimed\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m     39\u001b[0m     ts \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 40\u001b[0m     result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m     41\u001b[0m     te \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(te\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mts)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\MscThesis\\my_utils.py:227\u001b[0m, in \u001b[0;36mcl_simple_train_loop\u001b[1;34m(bm, cl_strategy, model, optimizer, number_of_workers, classes, scr)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28mprint\u001b[39m(classes)\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# Create a DataFrame for seaborn violin plot\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m weight_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassification_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# Set up seaborn style\u001b[39;00m\n\u001b[0;32m    230\u001b[0m sns\u001b[38;5;241m.\u001b[39mset(style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhitegrid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\MscThesis\\lib\\site-packages\\pandas\\core\\frame.py:816\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    805\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    806\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    813\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[0;32m    814\u001b[0m         )\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    818\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\MscThesis\\lib\\site-packages\\pandas\\core\\internals\\construction.py:332\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    323\u001b[0m     values \u001b[38;5;241m=\u001b[39m sanitize_array(\n\u001b[0;32m    324\u001b[0m         values,\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    328\u001b[0m         allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m     )\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_get_axes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\MscThesis\\lib\\site-packages\\pandas\\core\\internals\\construction.py:756\u001b[0m, in \u001b[0;36m_get_axes\u001b[1;34m(N, K, index, columns)\u001b[0m\n\u001b[0;32m    754\u001b[0m     columns \u001b[38;5;241m=\u001b[39m default_index(K)\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 756\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43mensure_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, columns\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\MscThesis\\lib\\site-packages\\pandas\\core\\indexes\\base.py:7649\u001b[0m, in \u001b[0;36mensure_index\u001b[1;34m(index_like, copy)\u001b[0m\n\u001b[0;32m   7647\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Index(index_like, copy\u001b[38;5;241m=\u001b[39mcopy, tupleize_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   7648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 7649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\MscThesis\\lib\\site-packages\\pandas\\core\\indexes\\base.py:526\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[1;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[0;32m    523\u001b[0m         data \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39m_dtype_obj)\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_scalar(data):\n\u001b[1;32m--> 526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_scalar_data_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(np\u001b[38;5;241m.\u001b[39masarray(data), dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\MscThesis\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5289\u001b[0m, in \u001b[0;36mIndex._raise_scalar_data_error\u001b[1;34m(cls, data)\u001b[0m\n\u001b[0;32m   5284\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   5285\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   5286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_scalar_data_error\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data):\n\u001b[0;32m   5287\u001b[0m     \u001b[38;5;66;03m# We return the TypeError so that we can raise it from the constructor\u001b[39;00m\n\u001b[0;32m   5288\u001b[0m     \u001b[38;5;66;03m#  in order to keep mypy happy\u001b[39;00m\n\u001b[1;32m-> 5289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   5290\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(...) must be called with a collection of some \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5291\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(data)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28misinstance\u001b[39m(data,\u001b[38;5;250m \u001b[39mnp\u001b[38;5;241m.\u001b[39mgeneric)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mstr\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas passed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5293\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Index(...) must be called with a collection of some kind, 4 was passed"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "results=[]\n",
    "results = my_utils.cl_simple_train_loop(bm, cl_strategy, model, optimizer, number_of_workers,classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "taW1mJPKqLwz",
    "outputId": "8c8753d5-7fef-4dbe-9096-ab2b2b5039f4"
   },
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "my_utils.checkpoint(model, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_Naive_CL_1_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VmpqErprNUq2",
    "outputId": "060f7b64-22b1-43ab-ce2a-34b089e89267"
   },
   "outputs": [],
   "source": [
    "# SOME TEST PREDICTIONS\n",
    "my_utils.n_test_predictions(model, test_loader_av, classes, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nfxKV4NxtfAr",
    "outputId": "667232c3-a3c5-4601-e93b-ed9eebf43878"
   },
   "outputs": [],
   "source": [
    "# CALCULATE ALL PREDICTIONS\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "y_pred_list, y_true_list = my_utils.get_predictions(model, test_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXDVLw9Mtt4v"
   },
   "outputs": [],
   "source": [
    "# GENERATE CONFUSION MATRIX AND F1\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "id": "q4syrSY9t9N9",
    "outputId": "ab7af71c-e301-49c2-a0a8-0cdf621dd9f7"
   },
   "outputs": [],
   "source": [
    "# PLOT CM\n",
    "my_utils.plot_confusion_matrix(cm, classes, 'Results/cm_Naive_1_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbYSfxKDj3vj"
   },
   "outputs": [],
   "source": [
    "# PLOT F1\n",
    "my_utils.plot_f1_scores(f1, classes, 'Results/f1_scores_Naive_1_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_tSNE_data_embedding(model, train_loader_av, classes, 'Results/tSNE_LwF_1_0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LwF strategy with cross entropy loss and adam optimizer\n",
    "The experiments are done on a neural network starting with 2 output nodes and gradually adapting with each experience to 22, but also on a neural network with 22 output nodes and running over the experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING UP DEVICE AND INSTANCE OF MODEL CLASS\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = my_architectures.BaselineColorNet_resnet18()\n",
    "model.to(device)\n",
    "\n",
    "# log to text file\n",
    "text_logger = TextLogger(open(\"LwF_log.txt\", \"a\"))\n",
    "\n",
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()\n",
    "\n",
    "csv_logger = CSVLogger()\n",
    "\n",
    "classes = my_utils.get_classes_from_dir(train_dir_2)\n",
    "class_to_indx = my_utils.classes_to_indices(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    forgetting_metrics(experience=False, stream=True),\n",
    "    ram_usage_metrics(experience=True, stream=True, epoch=False),\n",
    "    bwt_metrics(experience=True, stream=True),\n",
    "    loggers=[interactive_logger, text_logger, csv_logger],\n",
    "    strict_checks=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER and LOSS CRITERION\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRATEGY \n",
    "# ALPHA AND TEMPERATURE are taken from the paper by Oren & Wolf - \"In defense of the Learning Without Forgetting for Task Incremental Learning\"\n",
    "cl_strategy = LwF(\n",
    "    model=model, optimizer=optimizer, criterion=criterion, alpha=0.25, temperature=2.0, \n",
    "    train_mb_size=64, train_epochs=20, eval_mb_size=32, device=device, evaluator=eval_plugin, \n",
    "    plugins=[ReplayPlugin(mem_size=2*len(train_set))] #Early stopping is not used here because LwF does not inherently support Early Stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "results=[]\n",
    "results = my_utils.cl_simple_train_loop(bm, cl_strategy, model, optimizer, number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "my_utils.checkpoint(model, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_LwF_CL_2_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_LwF_CL_2_0.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE ALL PREDICTIONS\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "y_pred_list, y_true_list = my_utils.get_predictions(model, test_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE CONFUSION MATRIX AND F1\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT CM\n",
    "my_utils.plot_confusion_matrix(cm, classes, 'Results/cm_LwF_2_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT F1\n",
    "my_utils.plot_f1_scores(f1, classes, 'Results/f1_scores_LwF_2_0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Oren & Wolf (\"In defense of the learning without forgetting for task incremental learning\"), LwF mostly benefits from a Wide-ResNet netwrok rather than from deeper networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_tSNE_data_embedding(model, train_loader_av, classes, 'Results/tSNE_LwF_2_0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGEM strategy with cross entropy loss and adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_architectures.BaselineColorNet_resnet18()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER AND LOSS CRITERION\n",
    "optimizer = AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = CrossEntropyLoss(label_smoothing=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRATEGY\n",
    "cl_strategy = AGEM(\n",
    "    model=model, optimizer=optimizer, criterion=criterion, patterns_per_exp=10,\n",
    "    train_mb_size=64, train_epochs=10, eval_mb_size=32, device=device, evaluator=eval_plugin,\n",
    "    plugins=[ReplayPlugin(mem_size=2*len(train_set)), AGEMPlugin(patterns_per_experience=11, sample_size=64)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "results=[]\n",
    "results = my_utils.cl_simple_train_loop(bm, cl_strategy, model, optimizer, number_of_workers, classes, scr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "my_utils.checkpoint(model, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_AGEM_CL_1_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE PREDICTIONS\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "y_pred_list, y_true_list = my_utils.get_predictions(model, test_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE CONFUSION MATRIX AND F1\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT CM\n",
    "my_utils.plot_confusion_matrix(cm, classes, 'cm_adaptive_AGEM_1_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT F1\n",
    "my_utils.plot_f1_scores(f1, classes, 'f1_adaptive_AGEM_1_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict = eval_plugin.get_all_metrics()\n",
    "#for key in metric_dict.keys():\n",
    "#    print(key)\n",
    "metric_dict['Top1_Acc_Stream/eval_phase/train_stream/Task000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(metric_dict['Loss_Stream/eval_phase/train_stream/Task000'][1])\n",
    "plt.title('Loss Stream')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['epoch'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## AR1 strategy with cross entropy loss and adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_architectures.BaselineColorNet_resnet18()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS CRITERION\n",
    "# AR1 uses SGD, there is no option to change the optimizer\n",
    "criterion = CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32), label_smoothing=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AR1 STRATEGY\n",
    "cl_strategy = AR1(criterion=CrossEntropyLoss(), lr=0.001, inc_lr=0.00005, init_update_rate = 0.01, train_epochs=20,\n",
    "                  l2 = 0.0005, inc_update_rate = 0.00005, inc_step = 4.1e-05, rm_sz = 2*len(train_set), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "results=[]\n",
    "results = my_utils.cl_simple_train_loop(bm, cl_strategy, model, optimizer, number_of_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "my_utils.checkpoint(model, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_AR1_CL_2_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE PREDICTIONS\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "y_pred_list, y_true_list = my_utils.get_predictions(model, test_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE CONFUSION MATRIX AND F1\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT CM\n",
    "my_utils.plot_confusion_matrix(cm, classes, 'cm_AR1_2_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT F1\n",
    "my_utils.plot_f1_scores(f1, classes, 'f1_AR1_2_0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK INCREMENTAL - Instance based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_architectures.BaselineColorNet_resnet18(num_classes=10)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = my_dataloaders.GravitySpy_2_0_dataset(root=train_dir, cls=classes, transform=my_transformations.transformAV_224_Crop)\n",
    "val_set = my_dataloaders.GravitySpy_2_0_dataset(root=val_dir, cls=classes, transform=my_transformations.transformAV_224)\n",
    "test_set = my_dataloaders.GravitySpy_2_0_dataset(root=test_dir, cls=classes, transform=my_transformations.transformAV_224)\n",
    "\n",
    "train_set_av = AvalancheDataset(train_set)\n",
    "train_loader_av = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "\n",
    "val_set_av = AvalancheDataset(val_set)\n",
    "val_loader_av = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "\n",
    "test_set_av = AvalancheDataset(test_set)\n",
    "test_loader_av = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "\n",
    "train_set_av.targets = train_set.labels\n",
    "test_set_av.targets = test_set.labels\n",
    "val_set_av.targets = val_set.labels\n",
    "\n",
    "train_set_av.uniques = list(set(train_set.labels))\n",
    "test_set_av.uniques = list(set(test_set.labels))\n",
    "val_set_av.uniques = list(set(val_set.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE BENCHMARK\n",
    "# CL custom benchmark, here we opt for the generator New Instances (NI)\n",
    "# Given a sequence of train and test datasets this creates the continual stream as a series of experiences.\n",
    "bm = ni_benchmark(\n",
    "    train_dataset=train_set_av,\n",
    "    test_dataset=val_set_av,\n",
    "    n_experiences=5,\n",
    "    shuffle=True,\n",
    "    seed=1234,\n",
    "    task_labels=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE EVALUATION PLUGIN\n",
    "# The evaluation plugin manages the metrics computation. It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    forgetting_metrics(experience=False, stream=True),\n",
    "    ram_usage_metrics(experience=True, stream=True, epoch=False),\n",
    "    bwt_metrics(experience=True, stream=True),\n",
    "    loggers=[interactive_logger, text_logger, csv_logger],\n",
    "    strict_checks=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive strategy with Adam and Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER and CRITERION\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = CrossEntropyLoss(label_smoothing=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAIVE STRATEGY\n",
    "cl_strategy = Naive(\n",
    "    model=model, optimizer=optimizer, criterion=criterion, train_mb_size=32, train_epochs=20, \n",
    "    eval_mb_size=16, device=device, evaluator=eval_plugin, \n",
    "    plugins=[ReplayPlugin(mem_size=2*len(train_set)), EarlyStoppingPlugin(patience=2, val_stream_name='valid')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "results=[]\n",
    "results = my_utils.cl_simple_train_loop(bm, cl_strategy, model, optimizer, number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "my_utils.checkpoint(model, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_TaskBased_Naive_CL_2_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.load('C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_TaskBased_Naive_CL_2_0.pth')\n",
    "model.load_state_dict(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE ALL PREDICTIONS\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "y_pred_list, y_true_list = my_utils.get_predictions(model, test_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE CONFUSION MATRIX AND F1\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT CM\n",
    "my_utils.plot_confusion_matrix(cm, classes, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Results\\\\cm_TaskBased_Naive_2_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT F1\n",
    "my_utils.plot_f1_scores(f1, classes, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Results\\\\f1_TaskBased_Naive_2_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_tSNE_data_embedding(model, train_loader_av, classes, 'Results/tSNE_taskBased_Naive_2_0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCR (Supervised Contrastive Replay) \n",
    "Memory buffer: 1k, 2k\n",
    "Model = ResNet18\n",
    "Loss = SCR loss / Cross-entropy loss\n",
    "Optimizer = SGD\n",
    "Temperature = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.training import SCR, SCRLoss\n",
    "from avalanche.models import SCRModel\n",
    "resnet18 = my_architectures.BaselineColorNet_resnet18(num_classes=10)\n",
    "projection_network = nn.Linear(10, 512)  # Adjust the output size as needed\n",
    "scr_model = SCRModel(feature_extractor=resnet18, projection=projection_network)\n",
    "optimizer = SGD(model.parameters(), lr=0.001, weight_decay=1e-5) #although in the paper lr = 0.1 initially\n",
    "criterion = CrossEntropyLoss(label_smoothing=0.2)\n",
    "scr_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_model.feature_extractor.fc3.weight.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE EVALUATION PLUGIN\n",
    "# The evaluation plugin manages the metrics computation. It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "# Accuracy cannot be monitored during training of SCR. During training NCRLoss is monitored\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=False, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=False, epoch=False, experience=True, stream=True),\n",
    "    #forgetting_metrics(experience=False, stream=True),\n",
    "    #bwt_metrics(experience=True, stream=True),\n",
    "    #forward_transfer_metrics(experience=True, stream=True),\n",
    "    loggers=[InteractiveLogger()],\n",
    "    strict_checks=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCR STRATEGY\n",
    "cl_strategy = SCR(\n",
    "    model=scr_model, optimizer=optimizer, criterion=criterion, train_mb_size=32, train_epochs=2, \n",
    "    eval_mb_size=16, mem_size=1000, temperature=0.1, device=device, evaluator=eval_plugin\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "results=[]\n",
    "results = my_utils.cl_simple_train_loop(bm, cl_strategy, scr_model, optimizer, number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "my_utils.checkpoint(scr_model, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_TaskBased_Contrastive_CL_2_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE ALL PREDICTIONS\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "y_pred_list, y_true_list = my_utils.get_predictions(scr_model, test_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE CONFUSION MATRIX AND F1\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT CM\n",
    "my_utils.plot_confusion_matrix(cm, classes, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Results\\\\cm_TaskBased_Contrastive_2_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT F1\n",
    "my_utils.plot_f1_scores(f1, classes, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Results\\\\f1_TaskBased_Contrastive_2_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model\n",
    "scr_model.load_state_dict(torch.load('C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_TaskBased_Contrastive_CL_2_0.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model to gpu\n",
    "scr_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_tSNE_data_embedding(scr_model, train_loader_av, 'Results/tSNE_ScR_2_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second online TRAINING\n",
    "results=[]\n",
    "results = my_utils.cl_simple_train_loop(bm, cl_strategy, scr_model, optimizer, number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE ALL PREDICTIONS\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "y_pred_list, y_true_list = my_utils.get_predictions(scr_model, test_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE CONFUSION MATRIX AND F1\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "my_utils.checkpoint(scr_model, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_TaskBased_Contrastive_CL_2_0_SecondTraining.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT CM\n",
    "my_utils.plot_confusion_matrix(cm, classes, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Results\\\\cm_TaskBased_Contrastive_2_0_SecondTraining.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT F1\n",
    "my_utils.plot_f1_scores(f1, classes, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Results\\\\f1_TaskBased_Contrastive_2_0_SecondTraining.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.load('C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_TaskBased_Contrastive_CL_2_0_SecondTraining.pth')\n",
    "scr_model.load_state_dict(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_tSNE_data_embedding(temp, train_loader_av, classes, 'Results/tSNE_ScR_2_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOrI+t9FJjnAgoquNJb9Lug",
   "include_colab_link": true,
   "mount_file_id": "1vsLt8KULa-1HhNdKYHdrNM4CC6hJY3lU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
