{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/brianbaert/MscThesis/blob/main/MscThesis_Playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOAiIcv_lMu8"
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LiPWZMxjETE3",
    "outputId": "7c1608f9-ddbd-48dd-d90c-9f886d526af0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.14\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "#!pip install avalanche_lib[all]\n",
    "#!pip install gwpy\n",
    "#!pip install nds2\n",
    "#!pip show avalanche-lib\n",
    "#!pip show gwpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t1jbz0_g5bZW",
    "outputId": "68b123d7-8ec2-4e8c-d4da-9c025017c0f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%cd /content/drive/MyDrive/MscThesis\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "rVEI5x8RadVA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import DatasetFolder, ImageFolder\n",
    "from tqdm import tqdm\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, \\\n",
    "accuracy_score\n",
    "from avalanche.models import SlimResNet18, MTSlimResNet18, SimpleCNN\n",
    "from avalanche.models import as_multitask, IncrementalClassifier\n",
    "from avalanche.models.utils import avalanche_model_adaptation\n",
    "from avalanche.training import Naive, LwF, GenerativeReplay, ICaRLLossPlugin, ICaRL, EWC, AR1, LFL, AGEM\n",
    "from avalanche.logging import InteractiveLogger, TextLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin, ReplayPlugin, LwFPlugin, EarlyStoppingPlugin, AGEMPlugin\n",
    "from avalanche.training.plugins.lr_scheduling import LRSchedulerPlugin\n",
    "from avalanche.benchmarks import nc_benchmark, ni_benchmark\n",
    "from avalanche.benchmarks.utils import AvalancheDataset\n",
    "from avalanche.benchmarks.scenarios.dataset_scenario import benchmark_from_datasets\n",
    "from avalanche.benchmarks.scenarios.supervised import class_incremental_benchmark\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics,loss_metrics, \\\n",
    "timing_metrics, cpu_usage_metrics, StreamConfusionMatrix,disk_usage_metrics, gpu_usage_metrics, \\\n",
    "confusion_matrix_metrics, bwt_metrics, forward_transfer_metrics\n",
    "import multiprocessing as mp\n",
    "\n",
    "import my_utils\n",
    "import my_architectures\n",
    "import my_dataloaders\n",
    "import my_gwpy_and_fractals\n",
    "import my_transformations\n",
    "\n",
    "IMG_SIZE = (224,224)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# calculating the amount of workers usable\n",
    "number_of_workers = mp.cpu_count()\n",
    "number_of_workers = int(number_of_workers/2)\n",
    "print(number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Wpa8snbd2P8Q"
   },
   "outputs": [],
   "source": [
    "# Setting the correct directories\n",
    "train_dir = 'C:\\\\Users\\\\Brian.Baert\\\\TrainingSet'\n",
    "val_dir = 'C:\\\\Users\\\\Brian.Baert\\\\ValidationSet'\n",
    "test_dir = 'C:\\\\Users\\\\Brian.Baert\\\\ValidationSet' #This is not correct, but a temporary measure to test the models\n",
    "\n",
    "# Read in class labels\n",
    "class_file = open(\"classes.txt\", \"r\")\n",
    "classes = class_file.read()\n",
    "classes = classes.split(\", \")\n",
    "\n",
    "# Read in inverse weights used to cope with the imbalanced dataset\n",
    "#class_weights_file = open(\"class_weights.txt\", \"r\")\n",
    "#temp = class_weights_file.read()\n",
    "#temp_split = temp.split(\", \")\n",
    "#class_weights =  [float(weight) for weight in temp_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jz23eVLldHhF",
    "outputId": "b43f99a3-4c6f-4eb2-d62a-8608422b899f"
   },
   "outputs": [],
   "source": [
    "meta_train_v1 = pd.read_csv('C:\\\\Users\\\\Brian.Baert\\\\GravitySpy\\\\trainingset_v1d1_metadata.csv')\n",
    "print(meta_train_v1[meta_train_v1['sample_type']=='train']['label'].value_counts())\n",
    "classes = meta_train_v1['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CW-cEM0DJMom",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# DL Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5luuVVS8cUE"
   },
   "outputs": [],
   "source": [
    "train_set = my_dataloaders.GravitySpy_1_0_dataset(root=train_dir, cls=classes, transform=my_transformations.transformGray)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "val_set = my_dataloaders.GravitySpy_1_0_dataset(root=val_dir, cls=classes, transform=my_transformations.transformGray)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=True, num_workers=number_of_workers)\n",
    "test_set = my_dataloaders.GravitySpy_1_0_dataset(root=test_dir, cls=classes, transform=my_transformations.transformGray)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOyw0KMTCZQ2"
   },
   "outputs": [],
   "source": [
    "train_set = my_dataloaders.GravitySpy_0_5_dataset(root=train_dir, cls=classes, transform=my_transformations.transformGray)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "val_set = my_dataloaders.GravitySpy_0_5_dataset(root=val_dir, cls=classes, transform=my_transformations.transformGray)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=True)\n",
    "test_set = my_dataloaders.GravitySpy_0_5_dataset(root=test_dir, cls=classes, transform=my_transformations.transformGray)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XuoIZcjFLCSk",
    "outputId": "41bcc824-052b-42a4-e8d6-acd450325da6"
   },
   "outputs": [],
   "source": [
    "print(\"The training loader contains {} instances, the val loader contains {} instances and the test loader contains {} instances\".format(len(train_loader), len(val_loader), len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgVNrbCl_OQo",
    "outputId": "fe5d2346-f865-4985-bf03-e766da533cfe"
   },
   "outputs": [],
   "source": [
    "class_counts = train_set.count_class_instances()\n",
    "class_weights = []\n",
    "for class_name, count in class_counts.items():\n",
    "  print(f\"{class_name}: {count} instances\")\n",
    "  class_weights.append(1.0/(count/len(train_set)))\n",
    "\n",
    "class_weights\n",
    "\n",
    "with open(\"class_weights.txt\", \"w\") as output:\n",
    "    output.write(str(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akFKILwW9FFq",
    "outputId": "d27e49d1-6889-40ae-f8b7-fe65dc640487"
   },
   "outputs": [],
   "source": [
    "# Create Neural Network architecture for finetuning\n",
    "myNet = my_architectures.BaselineGrayscaleNet_resnet18()\n",
    "myNet.to(device)\n",
    "print(myNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BP8GXkixkmAS"
   },
   "outputs": [],
   "source": [
    "# Pytorch Training loop\n",
    "epoch_test_loss = 0\n",
    "epoch_test_acc = 0\n",
    "epoch_test_correct = 0\n",
    "epoch_test_total = 0\n",
    "best_vloss = 1_000_000.\n",
    "avg_loss = 0\n",
    "last_loss = 0\n",
    "timestamp = my_utils.get_timestamp()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32), label_smoothing=0.2)\n",
    "optimizer = Adam(myNet.parameters(), lr=0.001, weight_decay=1e-3) #LR from the study of Tiago Fernandes\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 10\n",
    "epoch_number = 0\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "  total_correct = 0\n",
    "  total_samples = 0\n",
    "  print('EPOCH {}: '.format(epoch_number + 1))\n",
    "  train_loss = 0.0\n",
    "  myNet.train(True)\n",
    "\n",
    "  ## TRAINING ONE EPOCH\n",
    "  for i, data in enumerate(train_loader, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    # forward + backward + optimize\n",
    "    outputs = myNet(inputs)\n",
    "    # compute loss and gradient\n",
    "    loss = criterion(outputs, labels)\n",
    "    #add part of regularization\n",
    "    l2_reg = torch.tensor(0.0)\n",
    "    for param in myNet.parameters():\n",
    "      if param.requires_grad: #exclude frozen layers\n",
    "        l2_reg += torch.norm(param, p=2)\n",
    "    loss += 1e-5 * l2_reg\n",
    "    #end part of regularization\n",
    "    loss.backward()\n",
    "    # adjust learning weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    train_loss += loss.item()\n",
    "    total_correct += (predicted == labels).sum().item()\n",
    "    total_samples += labels.size(0)\n",
    "\n",
    "    if i % 16 == 15:\n",
    "      last_loss = train_loss / 16\n",
    "      print('  batch {} loss: {}, acc: {}'.format(i+1, last_loss, 100*total_correct/total_samples))\n",
    "      tb_x = epoch * len(train_loader) + i + 1\n",
    "      train_loss=0.\n",
    "    avg_loss = last_loss\n",
    "\n",
    "  ## VALIDATION\n",
    "  running_vloss = 0.0\n",
    "  myNet.eval()\n",
    "\n",
    "  # disable gradient computation for validation\n",
    "  with torch.no_grad():\n",
    "    for i, vdata in enumerate(val_loader):\n",
    "      vinputs, vlabels = vdata\n",
    "      vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
    "      voutputs = myNet(vinputs)\n",
    "      vloss = criterion(voutputs, vlabels)\n",
    "      running_vloss += vloss\n",
    "  avg_vloss = running_vloss / (i+1)\n",
    "  print('LOSS train {:.4f} valid {:.4f} after {} seconds'.format(avg_loss, avg_vloss, time.time()-start))\n",
    "\n",
    "  if avg_vloss < best_vloss:\n",
    "    best_vloss = avg_vloss\n",
    "    model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "    torch.save(myNet.state_dict(), model_path)\n",
    "\n",
    "  #my_utils.checkpoint(myNet, f\"epoch-{epoch}.pth\")\n",
    "  epoch_number += 1\n",
    "\n",
    "end = time.time()\n",
    "print('Training finished after ', end-start, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFwrV-TIlBCz",
    "outputId": "5ce8f0e2-ed19-4fc1-d36c-9708971b38d8"
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "#torch.save(myNet.state_dict(), '/content/drive/MyDrive/MscThesis/thesis_baseline_finetune_0_5.pth')\n",
    "torch.save(myNet.state_dict(), 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_baseline_finetune_0_5.pth')\n",
    "print(\"Saved Pytorch Model state to thesis_baseline.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPb3Hcs4RbsK"
   },
   "outputs": [],
   "source": [
    "#myNet.load_state_dict(torch.load('/content/drive/MyDrive/MscThesis/thesis_baseline_finetune_1_0.pth'))\n",
    "myNet.load_state_dict(torch.load('C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_baseline_finetune_1_0.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijMNYNHxBB8D",
    "outputId": "c266284b-0852-40de-b6b7-31660d998d47"
   },
   "outputs": [],
   "source": [
    "my_utils.n_test_predictions(myNet, test_loader, classes, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPqfIK6qK_mg"
   },
   "outputs": [],
   "source": [
    "y_pred_list, y_true_list = my_utils.get_predictions(myNet, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETUYsXlKM2CB"
   },
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "from sklearn.metrics import f1_score\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nt51bp1_lJa-"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "# Use seaborn heatmap for visualization\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=classes, yticklabels=classes)\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "figTemp = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "figTemp.savefig('confusion_matrix_baseline_1_0.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyT7cWcJQAhm"
   },
   "outputs": [],
   "source": [
    "print(f\"F1 Score for each class: {f1}\")\n",
    "print(f\"The average F1 score is: {f1_score(y_true_list, y_pred_list, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qt4Bt4mURPMY"
   },
   "outputs": [],
   "source": [
    "# Create a horizontal bar plot for F1 scores with different colors\n",
    "plt.figure(figsize=(10,7))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(classes)))\n",
    "plt.barh(np.arange(len(classes)), f1, color=colors, align='center', alpha=0.5)\n",
    "plt.yticks(np.arange(len(classes)), classes)\n",
    "plt.xlabel('F1 Score')\n",
    "plt.title('F1 Score for Each Class')\n",
    "figTemp = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "figTemp.savefig('f1_scores_baseline_1_0.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPZtEpHZHjCZ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CL Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1) CLASS INCREMENTAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, dataloader, benchmark and eval plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiVzZcsjHlWV"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = my_architectures.BaselineColorNet_resnet18()\n",
    "model.to(device)\n",
    "\n",
    "classes = my_utils.get_classes_from_dir(train_dir2)\n",
    "class_to_indx = my_utils.classes_to_indices(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xaqHp2gOQb0j"
   },
   "outputs": [],
   "source": [
    "train_set = my_dataloaders.GravitySpy_2_0_dataset(root=train_dir, cls=classes, transform=my_transformations.transformAV_224_Crop)\n",
    "val_set = my_dataloaders.GravitySpy_2_0_dataset(root=val_dir, cls=classes, transform=my_transformations.transformAV_224)\n",
    "test_set = my_dataloaders.GravitySpy_2_0_dataset(root=test_dir, cls=classes, transform=my_transformations.transformAV_224)\n",
    "\n",
    "train_set_av = AvalancheDataset(train_set)\n",
    "train_loader_av = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "\n",
    "val_set_av = AvalancheDataset(val_set)\n",
    "val_loader_av = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "\n",
    "test_set_av = AvalancheDataset(test_set)\n",
    "test_loader_av = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "\n",
    "train_set_av.targets = train_set.labels\n",
    "test_set_av.targets = test_set.labels\n",
    "val_set_av.targets = val_set.labels\n",
    "\n",
    "train_set_av.uniques = list(set(train_set.labels))\n",
    "test_set_av.uniques = list(set(test_set.labels))\n",
    "val_set_av.uniques = list(set(val_set.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEhQaFGEQcTz"
   },
   "outputs": [],
   "source": [
    "# DEFINE THE BENCHMARK\n",
    "# CL custom benchmark, here we opt for the generator New Classes (NC)\n",
    "# Given a sequence of train and test datasets this creates the continual stream as a series of experiences.\n",
    "bm = nc_benchmark(\n",
    "    train_dataset=train_set_av,\n",
    "    test_dataset=val_set_av,\n",
    "    n_experiences=11,\n",
    "    shuffle=True,\n",
    "    seed=1234,\n",
    "    task_labels=False,\n",
    "    class_ids_from_zero_in_each_exp=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE EVALUATION PLUGIN\n",
    "# The evaluation plugin manages the metrics computation. It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    forgetting_metrics(experience=False, stream=True),\n",
    "    bwt_metrics(experience=True, stream=True),\n",
    "    forward_transfer_metrics(experience=True, stream=True),\n",
    "    loggers=[InteractiveLogger()],\n",
    "    strict_checks=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.to(device)\n",
    "#interactive_logger = InteractiveLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_first_image(train_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_first_image(val_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_first_image(test_loader_av)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Naive strategy with cross entropy loss and adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yK88QPCmQzA_",
    "outputId": "88bc7c53-2fa7-4199-fd80-553531c49ced"
   },
   "outputs": [],
   "source": [
    "# OPTIMIZER and CRITERION\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32), label_smoothing=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAIVE STRATEG\n",
    "cl_strategy = Naive(\n",
    "    model=model, optimizer=optimizer, criterion=criterion, train_mb_size=32, train_epochs=20, \n",
    "    eval_mb_size=16, device=device, evaluator=eval_plugin, \n",
    "    plugins=[ReplayPlugin(mem_size=2*len(train_set)), EarlyStoppingPlugin(patience=2, val_stream_name='valid')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "results=[]\n",
    "results = my_utils.cl_simple_train_loop(bm, cl_strategy, model, optimizer, number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "taW1mJPKqLwz",
    "outputId": "8c8753d5-7fef-4dbe-9096-ab2b2b5039f4"
   },
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "my_utils.checkpoint(model, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_Naive_CL_1_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VmpqErprNUq2",
    "outputId": "060f7b64-22b1-43ab-ce2a-34b089e89267"
   },
   "outputs": [],
   "source": [
    "# SOME TEST PREDICTIONS\n",
    "my_utils.n_test_predictions(model, test_loader_av, classes, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nfxKV4NxtfAr",
    "outputId": "667232c3-a3c5-4601-e93b-ed9eebf43878"
   },
   "outputs": [],
   "source": [
    "# CALCULATE ALL PREDICTIONS\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "y_pred_list, y_true_list = my_utils.get_predictions(model, test_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXDVLw9Mtt4v"
   },
   "outputs": [],
   "source": [
    "# GENERATE CONFUSION MATRIX AND F1\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "id": "q4syrSY9t9N9",
    "outputId": "ab7af71c-e301-49c2-a0a8-0cdf621dd9f7"
   },
   "outputs": [],
   "source": [
    "# PLOT CM\n",
    "my_utils.plot_confusion_matrix(cm, classes, 'cm_Naive_1_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbYSfxKDj3vj"
   },
   "outputs": [],
   "source": [
    "# PLOT F1\n",
    "my_utils.plot_f1_scores(f1, classes, 'f1_scores_Naive_1_0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LwF strategy with cross entropy loss and adam optimizer\n",
    "The experiments are done on a neural network starting with 2 output nodes and gradually adapting with each experience to 22, but also on a neural network with 22 output nodes and running over the experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING UP DEVICE AND INSTANCE OF MODEL CLASS\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = my_architectures.BaselineColorNet_resnet18()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER and LOSS CRITERION\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32), label_smoothing=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRATEGY \n",
    "# ALPHA AND TEMPERATURE are taken from the paper by Oren & Wolf - \"In defense of the Learning Without Forgetting for Task Incremental Learning\"\n",
    "cl_strategy = LwF(\n",
    "    model=model, optimizer=optimizer, criterion=criterion, alpha=0.25, temperature=2.0, \n",
    "    train_mb_size=64, train_epochs=20, eval_mb_size=32, device=device, evaluator=eval_plugin, \n",
    "    plugins=[ReplayPlugin(mem_size=2*len(train_set))] #Early stopping is not used here because LwF does not inherently support Early Stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "results=[]\n",
    "results = my_utils.cl_simple_train_loop(bm, cl_strategy, model, optimizer, number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "my_utils.checkpoint(model, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_LwF_CL_1_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE ALL PREDICTIONS\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "y_pred_list, y_true_list = my_utils.get_predictions(model, test_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE CONFUSION MATRIX AND F1\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT CM\n",
    "my_utils.plot_confusion_matrix(cm, classes, 'cm_LwF_1_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT F1\n",
    "my_utils.plot_f1_scores(f1, classes, 'f1_scores_LwF_1_0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Oren & Wolf (\"In defense of the learning without forgetting for task incremental learning\"), LwF mostly benefits from a Wide-ResNet netwrok rather than from deeper networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## AGEM strategy with cross entropy loss and adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_architectures.BaselineColorNet_resnet18()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER AND LOSS CRITERION\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32), label_smoothing=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRATEGY\n",
    "cl_strategy = AGEM(\n",
    "    model=model, optimizer=optimizer, criterion=criterion, patterns_per_exp=11,\n",
    "    train_mb_size=64, train_epochs=20, eval_mb_size=32, device=device, evaluator=eval_plugin,\n",
    "    plugins=[ReplayPlugin(mem_size=2*len(train_set)), AGEMPlugin(patterns_per_experience=11, sample_size=64)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "results=[]\n",
    "results = my_utils.cl_simple_train_loop(bm, cl_strategy, model, optimizer, number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "my_utils.checkpoint(model, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_AGEM_CL_1_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE PREDICTIONS\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "y_pred_list, y_true_list = my_utils.get_predictions(model, test_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE CONFUSION MATRIX AND F1\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT CM\n",
    "my_utils.plot_confusion_matrix(cm, classes, 'cm_adaptive_AGEM_1_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT F1\n",
    "my_utils.plot_f1_scores(f1, classes, 'f1_adaptive_AGEM_1_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict = eval_plugin.get_all_metrics()\n",
    "#for key in metric_dict.keys():\n",
    "#    print(key)\n",
    "metric_dict['Top1_Acc_Stream/eval_phase/train_stream/Task000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(metric_dict['Loss_Stream/eval_phase/train_stream/Task000'][1])\n",
    "plt.title('Loss Stream')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['epoch'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## AR1 strategy with cross entropy loss and adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_architectures.BaselineColorNet_resnet18()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS CRITERION\n",
    "# AR1 uses SGD, there is no option to change the optimizer\n",
    "criterion = CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32), label_smoothing=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AR1 STRATEGY\n",
    "cl_strategy = AR1(criterion=CrossEntropyLoss(), lr=0.001, inc_lr=0.00005, init_update_rate = 0.01, train_epochs=20,\n",
    "                  l2 = 0.0005, inc_update_rate = 0.00005, inc_step = 4.1e-05, rm_sz = 2*len(train_set), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "results=[]\n",
    "results = my_utils.cl_simple_train_loop(bm, cl_strategy, model, optimizer, number_of_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "my_utils.checkpoint(model, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_AR1_CL_2_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE PREDICTIONS\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "y_pred_list, y_true_list = my_utils.get_predictions(model, test_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE CONFUSION MATRIX AND F1\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT CM\n",
    "my_utils.plot_confusion_matrix(cm, classes, 'cm_AR1_2_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT F1\n",
    "my_utils.plot_f1_scores(f1, classes, 'f1_AR1_2_0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK INCREMENTAL - Instance based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_architectures.BaselineColorNet_resnet18(num_classes=10)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = my_dataloaders.GravitySpy_2_0_dataset(root=train_dir, cls=classes, transform=my_transformations.transformAV_224_Crop)\n",
    "val_set = my_dataloaders.GravitySpy_2_0_dataset(root=val_dir, cls=classes, transform=my_transformations.transformAV_224)\n",
    "test_set = my_dataloaders.GravitySpy_2_0_dataset(root=test_dir, cls=classes, transform=my_transformations.transformAV_224)\n",
    "\n",
    "train_set_av = AvalancheDataset(train_set)\n",
    "train_loader_av = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "\n",
    "val_set_av = AvalancheDataset(val_set)\n",
    "val_loader_av = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "\n",
    "test_set_av = AvalancheDataset(test_set)\n",
    "test_loader_av = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "\n",
    "train_set_av.targets = train_set.labels\n",
    "test_set_av.targets = test_set.labels\n",
    "val_set_av.targets = val_set.labels\n",
    "\n",
    "train_set_av.uniques = list(set(train_set.labels))\n",
    "test_set_av.uniques = list(set(test_set.labels))\n",
    "val_set_av.uniques = list(set(val_set.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE BENCHMARK\n",
    "# CL custom benchmark, here we opt for the generator New Instances (NI)\n",
    "# Given a sequence of train and test datasets this creates the continual stream as a series of experiences.\n",
    "bm = ni_benchmark(\n",
    "    train_dataset=train_set_av,\n",
    "    test_dataset=val_set_av,\n",
    "    n_experiences=5,\n",
    "    shuffle=True,\n",
    "    seed=1234,\n",
    "    task_labels=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE EVALUATION PLUGIN\n",
    "# The evaluation plugin manages the metrics computation. It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    forgetting_metrics(experience=False, stream=True),\n",
    "    bwt_metrics(experience=True, stream=True),\n",
    "    #forward_transfer_metrics(experience=True, stream=True),\n",
    "    loggers=[InteractiveLogger()],\n",
    "    strict_checks=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive strategy with Adam and Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER and CRITERION\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = CrossEntropyLoss(label_smoothing=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAIVE STRATEGY\n",
    "cl_strategy = Naive(\n",
    "    model=model, optimizer=optimizer, criterion=criterion, train_mb_size=32, train_epochs=20, \n",
    "    eval_mb_size=16, device=device, evaluator=eval_plugin, \n",
    "    plugins=[ReplayPlugin(mem_size=2*len(train_set)), EarlyStoppingPlugin(patience=2, val_stream_name='valid')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment with strategy: <avalanche.training.supervised.strategy_wrappers.Naive object at 0x00000191342A2C50>\n",
      "Start of experience:  0\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "10\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 13/13 [00:50<00:00,  3.86s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9036\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4450\n",
      "100%|██████████| 13/13 [00:57<00:00,  4.46s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.4219\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.7500\n",
      "100%|██████████| 13/13 [01:00<00:00,  4.64s/it]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.2695\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8275\n",
      "100%|██████████| 13/13 [01:15<00:00,  5.81s/it]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.1616\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8900\n",
      "100%|██████████| 13/13 [01:05<00:00,  5.02s/it]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.0754\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9325\n",
      "100%|██████████| 13/13 [01:01<00:00,  4.73s/it]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.0435\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9550\n",
      "100%|██████████| 13/13 [00:56<00:00,  4.33s/it]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.0259\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9550\n",
      "100%|██████████| 13/13 [00:58<00:00,  4.47s/it]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.0042\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9625\n",
      "100%|██████████| 13/13 [01:00<00:00,  4.69s/it]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9739\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9850\n",
      "100%|██████████| 13/13 [01:00<00:00,  4.65s/it]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9733\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9700\n",
      "100%|██████████| 13/13 [00:53<00:00,  4.08s/it]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9805\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9825\n",
      "100%|██████████| 13/13 [00:55<00:00,  4.28s/it]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9943\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9625\n",
      "100%|██████████| 13/13 [00:59<00:00,  4.57s/it]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9694\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9750\n",
      "100%|██████████| 13/13 [00:54<00:00,  4.16s/it]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9608\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9825\n",
      "100%|██████████| 13/13 [00:52<00:00,  4.04s/it]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9496\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9800\n",
      "100%|██████████| 13/13 [00:52<00:00,  4.05s/it]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9310\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9950\n",
      "100%|██████████| 13/13 [00:52<00:00,  4.04s/it]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9241\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 13/13 [00:52<00:00,  4.07s/it]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9256\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9975\n",
      "100%|██████████| 13/13 [00:53<00:00,  4.15s/it]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9176\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 13/13 [00:52<00:00,  4.06s/it]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9090\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from train stream --\n",
      "100%|██████████| 25/25 [00:22<00:00,  1.13it/s]\n",
      "> Eval on experience 0 (Task 0) from train stream ended.\n",
      "\tLoss_Exp/eval_phase/train_stream/Task000/Exp000 = 0.8752\n",
      "\tTop1_Acc_Exp/eval_phase/train_stream/Task000/Exp000 = 1.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/train_stream/Task000 = 0.8752\n",
      "\tStreamBWT/eval_phase/train_stream = 0.0000\n",
      "\tStreamForgetting/eval_phase/train_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/train_stream/Task000 = 1.0000\n",
      "Start of experience:  1\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "10\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 13/13 [01:42<00:00,  7.87s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9847\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9600\n",
      "100%|██████████| 13/13 [01:42<00:00,  7.86s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9528\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9775\n",
      "100%|██████████| 13/13 [01:39<00:00,  7.69s/it]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9306\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9900\n",
      "100%|██████████| 13/13 [01:40<00:00,  7.75s/it]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9278\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9938\n",
      "100%|██████████| 13/13 [01:42<00:00,  7.89s/it]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9215\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9888\n",
      "100%|██████████| 13/13 [01:40<00:00,  7.71s/it]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9268\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9900\n",
      "100%|██████████| 13/13 [01:40<00:00,  7.76s/it]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9250\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9912\n",
      "100%|██████████| 13/13 [01:40<00:00,  7.70s/it]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9127\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9962\n",
      "100%|██████████| 13/13 [01:41<00:00,  7.84s/it]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9092\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9975\n",
      "100%|██████████| 13/13 [01:40<00:00,  7.76s/it]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9060\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9988\n",
      "100%|██████████| 13/13 [01:41<00:00,  7.82s/it]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9112\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9962\n",
      "100%|██████████| 13/13 [01:39<00:00,  7.65s/it]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9175\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9900\n",
      "100%|██████████| 13/13 [01:42<00:00,  7.91s/it]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9238\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9938\n",
      "100%|██████████| 13/13 [01:41<00:00,  7.82s/it]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9139\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9962\n",
      "100%|██████████| 13/13 [01:48<00:00,  8.31s/it]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9102\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9988\n",
      "100%|██████████| 13/13 [01:54<00:00,  8.80s/it]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9068\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9988\n",
      "100%|██████████| 13/13 [01:51<00:00,  8.57s/it]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9050\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9988\n",
      "100%|██████████| 13/13 [01:40<00:00,  7.73s/it]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9049\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9975\n",
      "100%|██████████| 13/13 [01:41<00:00,  7.84s/it]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9031\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9988\n",
      "100%|██████████| 13/13 [01:48<00:00,  8.37s/it]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9049\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9988\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 1 (Task 0) from train stream --\n",
      "100%|██████████| 25/25 [00:22<00:00,  1.10it/s]\n",
      "> Eval on experience 1 (Task 0) from train stream ended.\n",
      "\tLoss_Exp/eval_phase/train_stream/Task000/Exp001 = 0.8746\n",
      "\tTop1_Acc_Exp/eval_phase/train_stream/Task000/Exp001 = 1.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/train_stream/Task000 = 0.8746\n",
      "\tStreamBWT/eval_phase/train_stream = 0.0000\n",
      "\tStreamForgetting/eval_phase/train_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/train_stream/Task000 = 1.0000\n",
      "Start of experience:  2\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "10\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 13/13 [01:49<00:00,  8.43s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9698\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9669\n",
      "100%|██████████| 13/13 [01:50<00:00,  8.51s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9529\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9779\n",
      "100%|██████████| 13/13 [01:51<00:00,  8.59s/it]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9345\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9877\n",
      "100%|██████████| 13/13 [01:50<00:00,  8.48s/it]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9343\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9804\n",
      "100%|██████████| 13/13 [01:50<00:00,  8.52s/it]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9312\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9877\n",
      "100%|██████████| 13/13 [01:53<00:00,  8.73s/it]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9114\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9939\n",
      "100%|██████████| 13/13 [01:50<00:00,  8.49s/it]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9156\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9926\n",
      "100%|██████████| 13/13 [01:49<00:00,  8.45s/it]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9104\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9951\n",
      "100%|██████████| 13/13 [01:48<00:00,  8.36s/it]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9016\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 13/13 [01:51<00:00,  8.59s/it]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9077\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9939\n",
      "100%|██████████| 13/13 [01:56<00:00,  8.94s/it]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9081\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9951\n",
      "100%|██████████| 13/13 [01:55<00:00,  8.87s/it]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9020\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 13/13 [01:59<00:00,  9.21s/it]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9059\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9988\n",
      "100%|██████████| 13/13 [01:56<00:00,  8.98s/it]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9051\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9975\n",
      "100%|██████████| 13/13 [01:51<00:00,  8.61s/it]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9021\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9975\n",
      "100%|██████████| 13/13 [01:46<00:00,  8.22s/it]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8998\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9988\n",
      "100%|██████████| 13/13 [01:42<00:00,  7.87s/it]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8973\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 13/13 [01:40<00:00,  7.76s/it]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8970\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 13/13 [01:43<00:00,  7.95s/it]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8977\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 13/13 [01:48<00:00,  8.34s/it]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8979\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 2 (Task 0) from train stream --\n",
      "100%|██████████| 25/25 [00:21<00:00,  1.14it/s]\n",
      "> Eval on experience 2 (Task 0) from train stream ended.\n",
      "\tLoss_Exp/eval_phase/train_stream/Task000/Exp002 = 0.8687\n",
      "\tTop1_Acc_Exp/eval_phase/train_stream/Task000/Exp002 = 1.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/train_stream/Task000 = 0.8687\n",
      "\tStreamBWT/eval_phase/train_stream = 0.0000\n",
      "\tStreamForgetting/eval_phase/train_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/train_stream/Task000 = 1.0000\n",
      "Start of experience:  3\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "10\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 13/13 [01:45<00:00,  8.12s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9815\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9620\n",
      "100%|██████████| 13/13 [01:42<00:00,  7.88s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9538\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9743\n",
      "100%|██████████| 13/13 [01:39<00:00,  7.65s/it]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9327\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9865\n",
      "100%|██████████| 13/13 [01:45<00:00,  8.09s/it]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9152\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9939\n",
      "100%|██████████| 13/13 [01:42<00:00,  7.85s/it]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9056\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9975\n",
      "100%|██████████| 13/13 [01:40<00:00,  7.70s/it]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9056\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9975\n",
      "100%|██████████| 13/13 [01:42<00:00,  7.90s/it]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9050\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9988\n",
      "100%|██████████| 13/13 [01:41<00:00,  7.79s/it]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9019\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9975\n",
      "100%|██████████| 13/13 [01:40<00:00,  7.70s/it]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8986\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 13/13 [01:40<00:00,  7.73s/it]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8985\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 13/13 [01:41<00:00,  7.77s/it]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8988\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9988\n",
      "100%|██████████| 13/13 [01:39<00:00,  7.68s/it]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8986\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 13/13 [01:41<00:00,  7.78s/it]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8988\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 13/13 [01:39<00:00,  7.66s/it]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8954\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 13/13 [01:41<00:00,  7.80s/it]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8944\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 13/13 [01:40<00:00,  7.73s/it]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8979\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 13/13 [01:42<00:00,  7.89s/it]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8945\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 13/13 [01:43<00:00,  7.96s/it]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8948\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 13/13 [01:44<00:00,  8.03s/it]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8960\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 13/13 [01:44<00:00,  8.03s/it]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8952\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 1.0000\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 3 (Task 0) from train stream --\n",
      "100%|██████████| 25/25 [00:22<00:00,  1.12it/s]\n",
      "> Eval on experience 3 (Task 0) from train stream ended.\n",
      "\tLoss_Exp/eval_phase/train_stream/Task000/Exp003 = 0.8691\n",
      "\tTop1_Acc_Exp/eval_phase/train_stream/Task000/Exp003 = 1.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/train_stream/Task000 = 0.8691\n",
      "\tStreamBWT/eval_phase/train_stream = 0.0000\n",
      "\tStreamForgetting/eval_phase/train_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/train_stream/Task000 = 1.0000\n",
      "Start of experience:  4\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "10\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 13/13 [01:51<00:00,  8.57s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9912\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9473\n",
      "100%|██████████| 13/13 [01:44<00:00,  8.06s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9680\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9657\n",
      "100%|██████████| 13/13 [01:47<00:00,  8.23s/it]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9521\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9730\n",
      "100%|██████████| 13/13 [01:52<00:00,  8.66s/it]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9300\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9853\n",
      "100%|██████████| 13/13 [01:53<00:00,  8.72s/it]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9329\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9804\n",
      "100%|██████████| 13/13 [01:55<00:00,  8.92s/it]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9353\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9792\n",
      "100%|██████████| 13/13 [02:02<00:00,  9.40s/it]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9307\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9828\n",
      "100%|██████████| 13/13 [02:04<00:00,  9.59s/it]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9276\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9877\n",
      "100%|██████████| 13/13 [02:11<00:00, 10.11s/it]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9158\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9914\n",
      "100%|██████████| 13/13 [03:05<00:00, 14.25s/it]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9023\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9988\n",
      "100%|██████████| 13/13 [03:04<00:00, 14.22s/it]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9040\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9988\n",
      "100%|██████████| 13/13 [02:15<00:00, 10.44s/it]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9026\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9975\n",
      "100%|██████████| 13/13 [02:17<00:00, 10.57s/it]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9034\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9963\n",
      "100%|██████████| 13/13 [02:17<00:00, 10.57s/it]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9081\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9951\n",
      "100%|██████████| 13/13 [02:15<00:00, 10.45s/it]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9037\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9963\n",
      " 69%|██████▉   | 9/13 [01:35<00:42, 10.61s/it]"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "results=[]\n",
    "results = my_utils.cl_simple_train_loop(bm, cl_strategy, model, optimizer, number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "my_utils.checkpoint(model, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_TaskBased_Naive_CL_2_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE ALL PREDICTIONS\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "y_pred_list, y_true_list = my_utils.get_predictions(model, test_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE CONFUSION MATRIX AND F1\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT CM\n",
    "my_utils.plot_confusion_matrix(cm, classes, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Results\\\\cm_TaskBased_Naive_2_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT F1\n",
    "my_utils.plot_f1_scores(f1, classes, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Results\\\\f1_TaskBased_Naive_2_0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCR (Supervised Contrastive Replay) \n",
    "Memory buffer: 1k, 2k\n",
    "Model = ResNet18\n",
    "Loss = SCR loss / Cross-entropy loss\n",
    "Optimizer = SGD\n",
    "Temperature = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.training import SCR, SCRLoss\n",
    "optimizer = SGD(model.parameters(), lr=0.001, weight_decay=1e-5) #although in the paper lr = 0.1 initially\n",
    "criterion = CrossEntropyLoss(label_smoothing=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE EVALUATION PLUGIN\n",
    "# The evaluation plugin manages the metrics computation. It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "# Accuracy cannot be monitored during training of SCR. During training NCRLoss is monitored\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=False, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    forgetting_metrics(experience=False, stream=True),\n",
    "    #bwt_metrics(experience=True, stream=True),\n",
    "    #forward_transfer_metrics(experience=True, stream=True),\n",
    "    loggers=[InteractiveLogger()],\n",
    "    strict_checks=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCR STRATEGY\n",
    "cl_strategy = SCR(\n",
    "    model=model, optimizer=optimizer, criterion=criterion, train_mb_size=32, train_epochs=20, \n",
    "    eval_mb_size=16, mem_size=1000, temperature=0.1, device=device, evaluator=eval_plugin\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOrI+t9FJjnAgoquNJb9Lug",
   "include_colab_link": true,
   "mount_file_id": "1vsLt8KULa-1HhNdKYHdrNM4CC6hJY3lU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
