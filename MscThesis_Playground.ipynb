{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/brianbaert/MscThesis/blob/main/MscThesis_Playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOAiIcv_lMu8"
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LiPWZMxjETE3",
    "outputId": "7c1608f9-ddbd-48dd-d90c-9f886d526af0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.14\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P6lu0g2-XF2A",
    "outputId": "a86324e6-29d5-46fd-f27b-ee9980e3b025"
   },
   "outputs": [],
   "source": [
    "#!pip install avalanche_lib[all]\n",
    "#!pip install gwpy\n",
    "#!pip install nds2\n",
    "!pip show avalanche-lib\n",
    "!pip show gwpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t1jbz0_g5bZW",
    "outputId": "68b123d7-8ec2-4e8c-d4da-9c025017c0f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #%cd /content/drive/MyDrive/MscThesis\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rVEI5x8RadVA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import DatasetFolder, ImageFolder\n",
    "from tqdm import tqdm\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, \\\n",
    "accuracy_score\n",
    "from avalanche.models import SlimResNet18, MTSlimResNet18, SimpleCNN\n",
    "from avalanche.models import as_multitask, IncrementalClassifier\n",
    "from avalanche.models.utils import avalanche_model_adaptation\n",
    "from avalanche.training import Naive, LwF, GenerativeReplay, ICaRLLossPlugin, ICaRL, EWC, AR1\n",
    "from avalanche.logging import InteractiveLogger, TextLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin, ReplayPlugin, LwFPlugin, EarlyStoppingPlugin\n",
    "from avalanche.training.supervised import VAETraining\n",
    "from avalanche.benchmarks import nc_benchmark\n",
    "from avalanche.benchmarks.utils import AvalancheDataset\n",
    "from avalanche.benchmarks.scenarios.dataset_scenario import benchmark_from_datasets\n",
    "from avalanche.benchmarks.scenarios.supervised import class_incremental_benchmark\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics,loss_metrics, \\\n",
    "timing_metrics, cpu_usage_metrics, StreamConfusionMatrix,disk_usage_metrics, gpu_usage_metrics, \\\n",
    "confusion_matrix_metrics\n",
    "import multiprocessing as mp\n",
    "\n",
    "import my_utils\n",
    "import my_architectures\n",
    "import my_dataloaders\n",
    "import my_gwpy_and_fractals\n",
    "\n",
    "IMG_SIZE = (224,224)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "number_of_workers = mp.cpu_count()\n",
    "number_of_workers = int(number_of_workers)\n",
    "print(number_of_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CW-cEM0DJMom",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# DL Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Wpa8snbd2P8Q"
   },
   "outputs": [],
   "source": [
    "#train_dir = '/content/drive/MyDrive/MscThesis/GravitySpy/train/train'\n",
    "train_dir2 = 'C:\\\\Users\\\\Brian.Baert\\\\TrainingSet'\n",
    "train_dir = 'C:\\\\Users\\\\Brian.Baert\\\\GravitySpy\\\\train\\\\train'\n",
    "#val_dir = '/content/drive/MyDrive/MscThesis/GravitySpy/validation/validation'\n",
    "val_dir = 'C:\\\\Users\\\\Brian.Baert\\\\GravitySpy\\\\validation\\\\validation'\n",
    "#test_dir = '/content/drive/MyDrive/MscThesis/GravitySpy/test/test'\n",
    "test_dir = 'C:\\\\Users\\\\Brian.Baert\\\\GravitySpy\\\\test\\\\test'\n",
    "#metadata_o3b = '/content/drive/MyDrive/MscThesis/data_o3b_high_confidence.csv'\n",
    "#metadata_o3a = '/content/drive/MyDrive/MscThesis/data_o3a_high_confidence.csv'\n",
    "#metadata_o1L1 = '/content/drive/MyDrive/MscThesis/GravitySpy/L1_O1.csv'\n",
    "#metadata_o2L1 = '/content/drive/MyDrive/MscThesis/GravitySpy/L1_O2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jz23eVLldHhF",
    "outputId": "b43f99a3-4c6f-4eb2-d62a-8608422b899f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "Blip                   1274\n",
      "Koi_Fish                498\n",
      "Low_Frequency_Burst     437\n",
      "Light_Modulation        361\n",
      "Extremely_Loud          316\n",
      "Low_Frequency_Lines     315\n",
      "Power_Line              314\n",
      "Scattered_Light         308\n",
      "Violin_Mode             284\n",
      "Scratchy                237\n",
      "1080Lines               229\n",
      "Whistle                 208\n",
      "Helix                   195\n",
      "Repeating_Blips         185\n",
      "No_Glitch               107\n",
      "Tomte                    73\n",
      "1400Ripples              59\n",
      "None_of_the_Above        57\n",
      "Air_Compressor           41\n",
      "Chirp                    41\n",
      "Wandering_Line           29\n",
      "Paired_Doves             19\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Whistle', '1080Lines', 'Blip', 'Violin_Mode',\n",
       "       'Scattered_Light', 'Power_Line', 'Light_Modulation',\n",
       "       'Koi_Fish', 'None_of_the_Above', 'Scratchy', 'Tomte',\n",
       "       'Chirp', 'Wandering_Line', 'Extremely_Loud',\n",
       "       'Repeating_Blips', 'No_Glitch', 'Low_Frequency_Lines',\n",
       "       'Paired_Doves', 'Low_Frequency_Burst', 'Helix',\n",
       "       'Air_Compressor', '1400Ripples'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_train_v1 = pd.read_csv('C:\\\\Users\\\\Brian.Baert\\\\GravitySpy\\\\trainingset_v1d1_metadata.csv')\n",
    "print(meta_train_v1[meta_train_v1['sample_type']=='train']['label'].value_counts())\n",
    "classes = meta_train_v1['label'].unique()\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kHz4V8FF7_AE"
   },
   "outputs": [],
   "source": [
    "classes = my_utils.get_classes_from_dir(train_dir2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gcZ3M2sA1q1W"
   },
   "outputs": [],
   "source": [
    "class_to_indx = {c: i for i, c in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9rKjn-MXm6SE"
   },
   "outputs": [],
   "source": [
    "transformGray = transforms.Compose([\n",
    "     transforms.Resize((140,170)),\n",
    "     transforms.Grayscale(num_output_channels=1),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5),(0.5))\n",
    "    ])\n",
    "\n",
    "transformRGB = transforms.Compose([\n",
    "     transforms.Resize((140,170)),\n",
    "     transforms.Grayscale(num_output_channels=3),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "transformAV = transforms.Compose([\n",
    "     transforms.Resize((32,32)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5),(0.5))\n",
    "    ])\n",
    "\n",
    "# Define the bounding box coordinates\n",
    "top, left, height, width = 60, 100, 580, 675\n",
    "\n",
    "# Create the custom crop transformation\n",
    "transformAV2 = transforms.Compose([\n",
    "    my_utils.CustomCrop(top, left, height, width),\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transformAV_AR1 = transforms.Compose([\n",
    "     transforms.Resize((224,224)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5),(0.5))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5luuVVS8cUE"
   },
   "outputs": [],
   "source": [
    "train_set = my_dataloaders.GravitySpy_1_0_dataset(root=train_dir, cls=classes, transform=transformGray)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "val_set = my_dataloaders.GravitySpy_1_0_dataset(root=val_dir, cls=classes, transform=transformGray)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=True, num_workers=number_of_workers)\n",
    "test_set = my_dataloaders.GravitySpy_1_0_dataset(root=test_dir, cls=classes, transform=transformGray)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOyw0KMTCZQ2"
   },
   "outputs": [],
   "source": [
    "train_set = my_dataloaders.GravitySpy_0_5_dataset(root=train_dir, cls=classes, transform=transformGray)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "val_set = my_dataloaders.GravitySpy_0_5_dataset(root=val_dir, cls=classes, transform=transformGray)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=True)\n",
    "test_set = my_dataloaders.GravitySpy_0_5_dataset(root=test_dir, cls=classes, transform=transformGray)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XuoIZcjFLCSk",
    "outputId": "41bcc824-052b-42a4-e8d6-acd450325da6"
   },
   "outputs": [],
   "source": [
    "print(\"The training loader contains {} instances, the val loader contains {} instances and the test loader contains {} instances\".format(len(train_loader), len(val_loader), len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgVNrbCl_OQo",
    "outputId": "fe5d2346-f865-4985-bf03-e766da533cfe"
   },
   "outputs": [],
   "source": [
    "class_counts = train_set.count_class_instances()\n",
    "class_weights = []\n",
    "for class_name, count in class_counts.items():\n",
    "  print(f\"{class_name}: {count} instances\")\n",
    "  class_weights.append(1.0/(count/len(train_set)))\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akFKILwW9FFq",
    "outputId": "d27e49d1-6889-40ae-f8b7-fe65dc640487"
   },
   "outputs": [],
   "source": [
    "# Create Neural Network architecture for finetuning\n",
    "myNet = my_architectures.BaselineGrayscaleNet_resnet18()\n",
    "myNet.to(device)\n",
    "print(myNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BP8GXkixkmAS"
   },
   "outputs": [],
   "source": [
    "# Pytorch Training loop\n",
    "epoch_test_loss = 0\n",
    "epoch_test_acc = 0\n",
    "epoch_test_correct = 0\n",
    "epoch_test_total = 0\n",
    "best_vloss = 1_000_000.\n",
    "avg_loss = 0\n",
    "last_loss = 0\n",
    "timestamp = my_utils.get_timestamp()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32), label_smoothing=0.2)\n",
    "optimizer = Adam(myNet.parameters(), lr=0.001, weight_decay=1e-3) #LR from the study of Tiago Fernandes\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 10\n",
    "epoch_number = 0\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "  total_correct = 0\n",
    "  total_samples = 0\n",
    "  print('EPOCH {}: '.format(epoch_number + 1))\n",
    "  train_loss = 0.0\n",
    "  myNet.train(True)\n",
    "\n",
    "  ## TRAINING ONE EPOCH\n",
    "  for i, data in enumerate(train_loader, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    # forward + backward + optimize\n",
    "    outputs = myNet(inputs)\n",
    "    # compute loss and gradient\n",
    "    loss = criterion(outputs, labels)\n",
    "    #add part of regularization\n",
    "    l2_reg = torch.tensor(0.0)\n",
    "    for param in myNet.parameters():\n",
    "      if param.requires_grad: #exclude frozen layers\n",
    "        l2_reg += torch.norm(param, p=2)\n",
    "    loss += 1e-5 * l2_reg\n",
    "    #end part of regularization\n",
    "    loss.backward()\n",
    "    # adjust learning weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    train_loss += loss.item()\n",
    "    total_correct += (predicted == labels).sum().item()\n",
    "    total_samples += labels.size(0)\n",
    "\n",
    "    if i % 16 == 15:\n",
    "      last_loss = train_loss / 16\n",
    "      print('  batch {} loss: {}, acc: {}'.format(i+1, last_loss, 100*total_correct/total_samples))\n",
    "      tb_x = epoch * len(train_loader) + i + 1\n",
    "      train_loss=0.\n",
    "    avg_loss = last_loss\n",
    "\n",
    "  ## VALIDATION\n",
    "  running_vloss = 0.0\n",
    "  myNet.eval()\n",
    "\n",
    "  # disable gradient computation for validation\n",
    "  with torch.no_grad():\n",
    "    for i, vdata in enumerate(val_loader):\n",
    "      vinputs, vlabels = vdata\n",
    "      vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
    "      voutputs = myNet(vinputs)\n",
    "      vloss = criterion(voutputs, vlabels)\n",
    "      running_vloss += vloss\n",
    "  avg_vloss = running_vloss / (i+1)\n",
    "  print('LOSS train {:.4f} valid {:.4f} after {} seconds'.format(avg_loss, avg_vloss, time.time()-start))\n",
    "\n",
    "  if avg_vloss < best_vloss:\n",
    "    best_vloss = avg_vloss\n",
    "    model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "    torch.save(myNet.state_dict(), model_path)\n",
    "\n",
    "  #my_utils.checkpoint(myNet, f\"epoch-{epoch}.pth\")\n",
    "  epoch_number += 1\n",
    "\n",
    "end = time.time()\n",
    "print('Training finished after ', end-start, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFwrV-TIlBCz",
    "outputId": "5ce8f0e2-ed19-4fc1-d36c-9708971b38d8"
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "#torch.save(myNet.state_dict(), '/content/drive/MyDrive/MscThesis/thesis_baseline_finetune_0_5.pth')\n",
    "torch.save(myNet.state_dict(), 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_baseline_finetune_0_5.pth')\n",
    "print(\"Saved Pytorch Model state to thesis_baseline.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPb3Hcs4RbsK"
   },
   "outputs": [],
   "source": [
    "#myNet.load_state_dict(torch.load('/content/drive/MyDrive/MscThesis/thesis_baseline_finetune_1_0.pth'))\n",
    "myNet.load_state_dict(torch.load('C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_baseline_finetune_1_0.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijMNYNHxBB8D",
    "outputId": "c266284b-0852-40de-b6b7-31660d998d47"
   },
   "outputs": [],
   "source": [
    "my_utils.n_test_predictions(myNet, test_loader, classes, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPqfIK6qK_mg"
   },
   "outputs": [],
   "source": [
    "y_pred_list, y_true_list = my_utils.get_predictions(myNet, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETUYsXlKM2CB"
   },
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "from sklearn.metrics import f1_score\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nt51bp1_lJa-"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "# Use seaborn heatmap for visualization\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=classes, yticklabels=classes)\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "figTemp = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "figTemp.savefig('confusion_matrix_baseline_1_0.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyT7cWcJQAhm"
   },
   "outputs": [],
   "source": [
    "print(f\"F1 Score for each class: {f1}\")\n",
    "print(f\"The average F1 score is: {f1_score(y_true_list, y_pred_list, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qt4Bt4mURPMY"
   },
   "outputs": [],
   "source": [
    "# Create a horizontal bar plot for F1 scores with different colors\n",
    "plt.figure(figsize=(10,7))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(classes)))\n",
    "plt.barh(np.arange(len(classes)), f1, color=colors, align='center', alpha=0.5)\n",
    "plt.yticks(np.arange(len(classes)), classes)\n",
    "plt.xlabel('F1 Score')\n",
    "plt.title('F1 Score for Each Class')\n",
    "figTemp = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "figTemp.savefig('f1_scores_baseline_1_0.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPZtEpHZHjCZ"
   },
   "source": [
    "# CL Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "JiVzZcsjHlWV"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = MTSlimResNet18(nclasses=22)\n",
    "#model = SlimResNet18(nclasses=2)\n",
    "model = SimpleCNN(num_classes=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Dropout(p=0.25, inplace=False)\n",
      "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Dropout(p=0.25, inplace=False)\n",
      "    (12): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): AdaptiveMaxPool2d(output_size=1)\n",
      "    (15): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (classifier): IncrementalClassifier(\n",
      "    (classifier): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.classifier = IncrementalClassifier(model.classifier[0].in_features, 1)\n",
    "model.to(device)\n",
    "interactive_logger = InteractiveLogger()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "xaqHp2gOQb0j"
   },
   "outputs": [],
   "source": [
    "train_set = my_dataloaders.GravitySpy_dataset(root=train_dir2, cls=classes, transform=transformAV2)\n",
    "val_set = my_dataloaders.GravitySpy_dataset(root=val_dir, cls=classes, transform=transformAV)\n",
    "test_set = my_dataloaders.GravitySpy_dataset(root=test_dir, cls=classes, transform=transformAV)\n",
    "\n",
    "#train_set = my_dataloaders.GravitySpy_1_0_dataset(root=train_dir, cls=classes, transform=transformAV2)\n",
    "#val_set = my_dataloaders.GravitySpy_1_0_dataset(root=val_dir, cls=classes, transform=transformAV)\n",
    "#test_set = my_dataloaders.GravitySpy_1_0_dataset(root=test_dir, cls=classes, transform=transformAV)\n",
    "\n",
    "#train_set = my_dataloaders.GravitySpy_0_5_dataset(root=train_dir, cls=classes, transform=transformAV)\n",
    "#val_set = my_dataloaders.GravitySpy_0_5_dataset(root=val_dir, cls=classes, transform=transformAV)\n",
    "#test_set = my_dataloaders.GravitySpy_0_5_dataset(root=test_dir, cls=classes, transform=transformAV)\n",
    "\n",
    "#train_set = my_dataloaders.GravitySpy_1_0_dataset(root=train_dir, cls=classes, transform=transformAV_AR1)\n",
    "#val_set = my_dataloaders.GravitySpy_1_0_dataset(root=val_dir, cls=classes, transform=transformAV_AR1)\n",
    "#test_set = my_dataloaders.GravitySpy_1_0_dataset(root=test_dir, cls=classes, transform=transformAV_AR1)\n",
    "\n",
    "train_set_av = AvalancheDataset(train_set)\n",
    "train_loader_av = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "\n",
    "val_set_av = AvalancheDataset(val_set)\n",
    "val_loader_av = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=True, num_workers=number_of_workers)\n",
    "\n",
    "test_set_av = AvalancheDataset(test_set)\n",
    "test_loader_av = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "train_set_av.targets = train_set.labels\n",
    "test_set_av.targets = test_set.labels\n",
    "val_set_av.targets = val_set.labels\n",
    "\n",
    "train_set_av.uniques = list(set(train_set.labels))\n",
    "test_set_av.uniques = list(set(test_set.labels))\n",
    "val_set_av.uniques = list(set(val_set.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "LEhQaFGEQcTz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm = nc_benchmark(\n",
    "    train_dataset=train_set_av,\n",
    "    test_dataset=val_set_av,\n",
    "    n_experiences=11,\n",
    "    shuffle=False,\n",
    "    seed=51,\n",
    "    task_labels=False,\n",
    "    class_ids_from_zero_in_each_exp=False,\n",
    ")\n",
    "\n",
    "bm.n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yK88QPCmQzA_",
    "outputId": "88bc7c53-2fa7-4199-fd80-553531c49ced"
   },
   "outputs": [],
   "source": [
    "# DEFINE THE EVALUATION PLUGIN\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(experience=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    confusion_matrix_metrics(num_classes=bm.n_classes, save_image=True, stream=True),\n",
    "    loggers=[InteractiveLogger()],\n",
    "    strict_checks=False,\n",
    ")\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "#criterion = CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32), label_smoothing=0.2)\n",
    "criterion = CrossEntropyLoss(label_smoothing=0.2)\n",
    "\n",
    "cl_strategy = Naive(\n",
    "#cl_strategy = LwF(\n",
    "#cl_strategy = ICaRL(\n",
    "#cl_strategy= EWC(\n",
    "#cl_strategy = AR1(\n",
    "#cl_strategy = VAETraining(\n",
    "    #model, optimizer, criterion, ewc_lambda=5.5, train_epochs=10, mode='online', decay_factor=0.05, keep_importance_data=True, train_mb_size=32, eval_mb_size=16, device=device\n",
    "    model, optimizer, criterion, train_mb_size=32, train_epochs=5, eval_mb_size=16, device=device, plugins=[eval_plugin, ReplayPlugin(mem_size=2000), EarlyStoppingPlugin(patience=2, val_stream_name='valid')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mTwMkUvKNfkw",
    "outputId": "f5ff3e10-78ee-4a76-e9dd-cb7bf15220fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=64, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.classifier.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not execute together with the next cell --> causes unwanted classifier adjustments\n",
    "for experience in bm.train_stream:\n",
    "    print(experience.classes_in_this_experience)\n",
    "    avalanche_model_adaptation(model.classifier, experience)\n",
    "    print(model.classifier.classifier)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IzTdOiHwMazz",
    "outputId": "4b5f8528-1e80-421f-d6ef-c758d2821ef6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience:  0\n",
      "Current Classes:  [0, 1]\n",
      "-- >> Start of training phase << --\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 13/13 [00:22<00:00,  1.76s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.6958\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.6936\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4975\n",
      "100%|██████████| 13/13 [00:22<00:00,  1.76s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.6958\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4975\n",
      "100%|██████████| 13/13 [00:22<00:00,  1.71s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.6936\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.6876\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.5200\n",
      "100%|██████████| 13/13 [00:22<00:00,  1.71s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.6936\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.5200\n",
      "100%|██████████| 13/13 [00:23<00:00,  1.81s/it]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.6937\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.6976\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4625\n",
      "100%|██████████| 13/13 [00:23<00:00,  1.81s/it]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.6937\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4625\n",
      "100%|██████████| 13/13 [00:23<00:00,  1.80s/it]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.6932\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.6929\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.5075\n",
      "100%|██████████| 13/13 [00:23<00:00,  1.80s/it]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.6932\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.5075\n",
      "100%|██████████| 13/13 [00:22<00:00,  1.72s/it]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.6941\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.6909\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4650\n",
      "100%|██████████| 13/13 [00:22<00:00,  1.72s/it]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.6941\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4650\n",
      "-- >> End of training phase << --\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Linear(in_features=64, out_features=2, bias=True)\n",
      "Start of experience:  1\n",
      "Current Classes:  [2, 3]\n",
      "-- >> Start of training phase << --\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 13/13 [00:25<00:00,  1.99s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.3878\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.3851\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2662\n",
      "100%|██████████| 13/13 [00:25<00:00,  1.99s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.3878\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2662\n",
      "100%|██████████| 13/13 [00:27<00:00,  2.08s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.3866\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.3874\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2612\n",
      "100%|██████████| 13/13 [00:27<00:00,  2.08s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.3866\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2612\n",
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in bm.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "    cl_strategy.train(experience, num_workers=4)\n",
    "    print('Training completed')\n",
    "    avalanche_model_adaptation(model.classifier, experience)\n",
    "    print(model.classifier.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "taW1mJPKqLwz",
    "outputId": "8c8753d5-7fef-4dbe-9096-ab2b2b5039f4"
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "torch.save(model.state_dict(), 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_Naive_CL_1_0.pth')\n",
    "#torch.save(model.state_dict(), '/content/drive/MyDrive/MscThesis/thesis_generative_CL_1_0.pth')\n",
    "print(\"Saved Pytorch Model state to thesis_Naive_CL_1_0.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6pNYVJcrJ_s-",
    "outputId": "2b566a7c-9c49-476d-ef74-284afbaa1eb8"
   },
   "outputs": [],
   "source": [
    "model = SimpleCNN(num_classes=22)\n",
    "#model.load_state_dict(torch.load('C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_Naive_CL_1_0.pth'))\n",
    "model.load_state_dict(torch.load('C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_Naive_CL_1_0.pth'))\n",
    "#model.load_state_dict(torch.load('/content/drive/MyDrive/MscThesis/thesis_generative_CL_1_0.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VmpqErprNUq2",
    "outputId": "060f7b64-22b1-43ab-ce2a-34b089e89267"
   },
   "outputs": [],
   "source": [
    "my_utils.n_test_predictions(model, test_loader_av, classes, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nfxKV4NxtfAr",
    "outputId": "667232c3-a3c5-4601-e93b-ed9eebf43878"
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "y_pred_list, y_true_list = my_utils.get_predictions(model, test_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXDVLw9Mtt4v"
   },
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "id": "q4syrSY9t9N9",
    "outputId": "ab7af71c-e301-49c2-a0a8-0cdf621dd9f7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "# Use seaborn heatmap for visualization\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=classes, yticklabels=classes)\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "figTemp = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "#figTemp.savefig('f1_scores_Naive_1_0.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbYSfxKDj3vj"
   },
   "outputs": [],
   "source": [
    "# Create a horizontal bar plot for F1 scores with different colors\n",
    "plt.figure(figsize=(10,7))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(classes)))\n",
    "plt.barh(np.arange(len(classes)), f1, color=colors, align='center', alpha=0.5)\n",
    "plt.yticks(np.arange(len(classes)), classes)\n",
    "plt.xlabel('F1 Score')\n",
    "plt.title('F1 Score for Each Class')\n",
    "figTemp = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "figTemp.savefig('f1_scores_Naive_1_0.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_strategy = LwF(\n",
    "    model, optimizer, criterion, alpha=0.5, temperature=2.0,train_mb_size=32, train_epochs=10, eval_mb_size=16, device=device, plugins=[eval_plugin, ReplayPlugin(mem_size=2000), EarlyStoppingPlugin(patience=2, val_stream_name='valid')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting experiment with LwF...')\n",
    "results = []\n",
    "for experience in bm.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "    cl_strategy.train(experience, num_workers=0)\n",
    "    print('Training completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "torch.save(model.state_dict(), 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_LwF_CL_1_0.pth')\n",
    "#torch.save(model.state_dict(), '/content/drive/MyDrive/MscThesis/thesis_generative_CL_1_0.pth')\n",
    "print(\"Saved Pytorch Model state to thesis_LwF_CL_1_0.pth\")\n",
    "\n",
    "#calculate predictions\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "y_pred_list, y_true_list = my_utils.get_predictions(model, test_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "# Use seaborn heatmap for visualization\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "figTemp = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "figTemp.savefig('f1_scores_LwF_1_0.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a horizontal bar plot for F1 scores with different colors\n",
    "plt.figure(figsize=(10,7))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(classes)))\n",
    "plt.barh(np.arange(len(classes)), f1, color=colors, align='center', alpha=0.5)\n",
    "plt.yticks(np.arange(len(classes)), classes)\n",
    "plt.xlabel('F1 Score')\n",
    "plt.title('F1 Score for Each Class')\n",
    "figTemp = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "figTemp.savefig('f1_scores_LwF_1_0.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.benchmarks import SplitMNIST\n",
    "from avalanche.models import IncrementalClassifier\n",
    "\n",
    "benchmark = SplitMNIST(11, shuffle=False, class_ids_from_zero_in_each_exp=False)\n",
    "model = IncrementalClassifier(in_features=784)\n",
    "\n",
    "print(model)\n",
    "for exp in benchmark.train_stream:\n",
    "    model.adaptation(exp)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOrI+t9FJjnAgoquNJb9Lug",
   "include_colab_link": true,
   "mount_file_id": "1vsLt8KULa-1HhNdKYHdrNM4CC6hJY3lU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
