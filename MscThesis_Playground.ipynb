{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/brianbaert/MscThesis/blob/main/MscThesis_Playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOAiIcv_lMu8"
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LiPWZMxjETE3",
    "outputId": "7c1608f9-ddbd-48dd-d90c-9f886d526af0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.14\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P6lu0g2-XF2A",
    "outputId": "a86324e6-29d5-46fd-f27b-ee9980e3b025"
   },
   "outputs": [],
   "source": [
    "#!pip install avalanche_lib[all]\n",
    "#!pip install gwpy\n",
    "#!pip install nds2\n",
    "!pip show avalanche-lib\n",
    "!pip show gwpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t1jbz0_g5bZW",
    "outputId": "68b123d7-8ec2-4e8c-d4da-9c025017c0f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #%cd /content/drive/MyDrive/MscThesis\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rVEI5x8RadVA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import DatasetFolder, ImageFolder\n",
    "from tqdm import tqdm\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, \\\n",
    "accuracy_score\n",
    "from avalanche.models import SlimResNet18, MTSlimResNet18, SimpleCNN\n",
    "from avalanche.models import as_multitask, IncrementalClassifier\n",
    "from avalanche.models.utils import avalanche_model_adaptation\n",
    "from avalanche.training import Naive, LwF, GenerativeReplay, ICaRLLossPlugin, ICaRL, EWC, AR1\n",
    "from avalanche.logging import InteractiveLogger, TextLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin, ReplayPlugin, LwFPlugin, EarlyStoppingPlugin\n",
    "from avalanche.training.supervised import VAETraining\n",
    "from avalanche.benchmarks import nc_benchmark\n",
    "from avalanche.benchmarks.utils import AvalancheDataset\n",
    "from avalanche.benchmarks.scenarios.dataset_scenario import benchmark_from_datasets\n",
    "from avalanche.benchmarks.scenarios.supervised import class_incremental_benchmark\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics,loss_metrics, \\\n",
    "timing_metrics, cpu_usage_metrics, StreamConfusionMatrix,disk_usage_metrics, gpu_usage_metrics, \\\n",
    "confusion_matrix_metrics\n",
    "import multiprocessing as mp\n",
    "\n",
    "import my_utils\n",
    "import my_architectures\n",
    "import my_dataloaders\n",
    "import my_gwpy_and_fractals\n",
    "import my_transformations\n",
    "\n",
    "IMG_SIZE = (224,224)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "number_of_workers = mp.cpu_count()\n",
    "number_of_workers = int(number_of_workers/2)\n",
    "print(number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Wpa8snbd2P8Q"
   },
   "outputs": [],
   "source": [
    "train_dir2 = 'C:\\\\Users\\\\Brian.Baert\\\\TrainingSet'\n",
    "train_dir = 'C:\\\\Users\\\\Brian.Baert\\\\GravitySpy\\\\train\\\\train'\n",
    "val_dir = 'C:\\\\Users\\\\Brian.Baert\\\\GravitySpy\\\\validation\\\\validation'\n",
    "test_dir = 'C:\\\\Users\\\\Brian.Baert\\\\GravitySpy\\\\test\\\\test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jz23eVLldHhF",
    "outputId": "b43f99a3-4c6f-4eb2-d62a-8608422b899f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "Blip                   1274\n",
      "Koi_Fish                498\n",
      "Low_Frequency_Burst     437\n",
      "Light_Modulation        361\n",
      "Extremely_Loud          316\n",
      "Low_Frequency_Lines     315\n",
      "Power_Line              314\n",
      "Scattered_Light         308\n",
      "Violin_Mode             284\n",
      "Scratchy                237\n",
      "1080Lines               229\n",
      "Whistle                 208\n",
      "Helix                   195\n",
      "Repeating_Blips         185\n",
      "No_Glitch               107\n",
      "Tomte                    73\n",
      "1400Ripples              59\n",
      "None_of_the_Above        57\n",
      "Air_Compressor           41\n",
      "Chirp                    41\n",
      "Wandering_Line           29\n",
      "Paired_Doves             19\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "meta_train_v1 = pd.read_csv('C:\\\\Users\\\\Brian.Baert\\\\GravitySpy\\\\trainingset_v1d1_metadata.csv')\n",
    "print(meta_train_v1[meta_train_v1['sample_type']=='train']['label'].value_counts())\n",
    "classes = meta_train_v1['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kHz4V8FF7_AE"
   },
   "outputs": [],
   "source": [
    "classes = my_utils.get_classes_from_dir(train_dir2)\n",
    "class_to_indx = my_utils.classes_to_indices(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CW-cEM0DJMom",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# DL Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5luuVVS8cUE"
   },
   "outputs": [],
   "source": [
    "train_set = my_dataloaders.GravitySpy_1_0_dataset(root=train_dir, cls=classes, transform=transformGray)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "val_set = my_dataloaders.GravitySpy_1_0_dataset(root=val_dir, cls=classes, transform=transformGray)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=True, num_workers=number_of_workers)\n",
    "test_set = my_dataloaders.GravitySpy_1_0_dataset(root=test_dir, cls=classes, transform=transformGray)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOyw0KMTCZQ2"
   },
   "outputs": [],
   "source": [
    "train_set = my_dataloaders.GravitySpy_0_5_dataset(root=train_dir, cls=classes, transform=transformGray)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "val_set = my_dataloaders.GravitySpy_0_5_dataset(root=val_dir, cls=classes, transform=transformGray)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=True)\n",
    "test_set = my_dataloaders.GravitySpy_0_5_dataset(root=test_dir, cls=classes, transform=transformGray)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XuoIZcjFLCSk",
    "outputId": "41bcc824-052b-42a4-e8d6-acd450325da6"
   },
   "outputs": [],
   "source": [
    "print(\"The training loader contains {} instances, the val loader contains {} instances and the test loader contains {} instances\".format(len(train_loader), len(val_loader), len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgVNrbCl_OQo",
    "outputId": "fe5d2346-f865-4985-bf03-e766da533cfe"
   },
   "outputs": [],
   "source": [
    "class_counts = train_set.count_class_instances()\n",
    "class_weights = []\n",
    "for class_name, count in class_counts.items():\n",
    "  print(f\"{class_name}: {count} instances\")\n",
    "  class_weights.append(1.0/(count/len(train_set)))\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akFKILwW9FFq",
    "outputId": "d27e49d1-6889-40ae-f8b7-fe65dc640487"
   },
   "outputs": [],
   "source": [
    "# Create Neural Network architecture for finetuning\n",
    "myNet = my_architectures.BaselineGrayscaleNet_resnet18()\n",
    "myNet.to(device)\n",
    "print(myNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BP8GXkixkmAS"
   },
   "outputs": [],
   "source": [
    "# Pytorch Training loop\n",
    "epoch_test_loss = 0\n",
    "epoch_test_acc = 0\n",
    "epoch_test_correct = 0\n",
    "epoch_test_total = 0\n",
    "best_vloss = 1_000_000.\n",
    "avg_loss = 0\n",
    "last_loss = 0\n",
    "timestamp = my_utils.get_timestamp()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32), label_smoothing=0.2)\n",
    "optimizer = Adam(myNet.parameters(), lr=0.001, weight_decay=1e-3) #LR from the study of Tiago Fernandes\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 10\n",
    "epoch_number = 0\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "  total_correct = 0\n",
    "  total_samples = 0\n",
    "  print('EPOCH {}: '.format(epoch_number + 1))\n",
    "  train_loss = 0.0\n",
    "  myNet.train(True)\n",
    "\n",
    "  ## TRAINING ONE EPOCH\n",
    "  for i, data in enumerate(train_loader, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    # forward + backward + optimize\n",
    "    outputs = myNet(inputs)\n",
    "    # compute loss and gradient\n",
    "    loss = criterion(outputs, labels)\n",
    "    #add part of regularization\n",
    "    l2_reg = torch.tensor(0.0)\n",
    "    for param in myNet.parameters():\n",
    "      if param.requires_grad: #exclude frozen layers\n",
    "        l2_reg += torch.norm(param, p=2)\n",
    "    loss += 1e-5 * l2_reg\n",
    "    #end part of regularization\n",
    "    loss.backward()\n",
    "    # adjust learning weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    train_loss += loss.item()\n",
    "    total_correct += (predicted == labels).sum().item()\n",
    "    total_samples += labels.size(0)\n",
    "\n",
    "    if i % 16 == 15:\n",
    "      last_loss = train_loss / 16\n",
    "      print('  batch {} loss: {}, acc: {}'.format(i+1, last_loss, 100*total_correct/total_samples))\n",
    "      tb_x = epoch * len(train_loader) + i + 1\n",
    "      train_loss=0.\n",
    "    avg_loss = last_loss\n",
    "\n",
    "  ## VALIDATION\n",
    "  running_vloss = 0.0\n",
    "  myNet.eval()\n",
    "\n",
    "  # disable gradient computation for validation\n",
    "  with torch.no_grad():\n",
    "    for i, vdata in enumerate(val_loader):\n",
    "      vinputs, vlabels = vdata\n",
    "      vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
    "      voutputs = myNet(vinputs)\n",
    "      vloss = criterion(voutputs, vlabels)\n",
    "      running_vloss += vloss\n",
    "  avg_vloss = running_vloss / (i+1)\n",
    "  print('LOSS train {:.4f} valid {:.4f} after {} seconds'.format(avg_loss, avg_vloss, time.time()-start))\n",
    "\n",
    "  if avg_vloss < best_vloss:\n",
    "    best_vloss = avg_vloss\n",
    "    model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "    torch.save(myNet.state_dict(), model_path)\n",
    "\n",
    "  #my_utils.checkpoint(myNet, f\"epoch-{epoch}.pth\")\n",
    "  epoch_number += 1\n",
    "\n",
    "end = time.time()\n",
    "print('Training finished after ', end-start, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFwrV-TIlBCz",
    "outputId": "5ce8f0e2-ed19-4fc1-d36c-9708971b38d8"
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "#torch.save(myNet.state_dict(), '/content/drive/MyDrive/MscThesis/thesis_baseline_finetune_0_5.pth')\n",
    "torch.save(myNet.state_dict(), 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_baseline_finetune_0_5.pth')\n",
    "print(\"Saved Pytorch Model state to thesis_baseline.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPb3Hcs4RbsK"
   },
   "outputs": [],
   "source": [
    "#myNet.load_state_dict(torch.load('/content/drive/MyDrive/MscThesis/thesis_baseline_finetune_1_0.pth'))\n",
    "myNet.load_state_dict(torch.load('C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_baseline_finetune_1_0.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijMNYNHxBB8D",
    "outputId": "c266284b-0852-40de-b6b7-31660d998d47"
   },
   "outputs": [],
   "source": [
    "my_utils.n_test_predictions(myNet, test_loader, classes, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPqfIK6qK_mg"
   },
   "outputs": [],
   "source": [
    "y_pred_list, y_true_list = my_utils.get_predictions(myNet, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETUYsXlKM2CB"
   },
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "from sklearn.metrics import f1_score\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nt51bp1_lJa-"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "# Use seaborn heatmap for visualization\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=classes, yticklabels=classes)\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "figTemp = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "figTemp.savefig('confusion_matrix_baseline_1_0.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyT7cWcJQAhm"
   },
   "outputs": [],
   "source": [
    "print(f\"F1 Score for each class: {f1}\")\n",
    "print(f\"The average F1 score is: {f1_score(y_true_list, y_pred_list, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qt4Bt4mURPMY"
   },
   "outputs": [],
   "source": [
    "# Create a horizontal bar plot for F1 scores with different colors\n",
    "plt.figure(figsize=(10,7))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(classes)))\n",
    "plt.barh(np.arange(len(classes)), f1, color=colors, align='center', alpha=0.5)\n",
    "plt.yticks(np.arange(len(classes)), classes)\n",
    "plt.xlabel('F1 Score')\n",
    "plt.title('F1 Score for Each Class')\n",
    "figTemp = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "figTemp.savefig('f1_scores_baseline_1_0.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPZtEpHZHjCZ"
   },
   "source": [
    "# CL Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, dataloader, benchmark and eval plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "JiVzZcsjHlWV"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = my_architectures.SimpleCNN_224by224()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "xaqHp2gOQb0j"
   },
   "outputs": [],
   "source": [
    "train_set = my_dataloaders.GravitySpy_1_0_dataset(root=train_dir2, cls=classes, transform=my_transformations.transformAV_224_Crop)\n",
    "val_set = my_dataloaders.GravitySpy_1_0_dataset(root=val_dir, cls=classes, transform=my_transformations.transformAV_224)\n",
    "test_set = my_dataloaders.GravitySpy_1_0_dataset(root=test_dir, cls=classes, transform=my_transformations.transformAV_224)\n",
    "\n",
    "train_set_av = AvalancheDataset(train_set)\n",
    "train_loader_av = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=number_of_workers)\n",
    "\n",
    "val_set_av = AvalancheDataset(val_set)\n",
    "val_loader_av = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "\n",
    "test_set_av = AvalancheDataset(test_set)\n",
    "test_loader_av = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "\n",
    "train_set_av.targets = train_set.labels\n",
    "test_set_av.targets = test_set.labels\n",
    "val_set_av.targets = val_set.labels\n",
    "\n",
    "train_set_av.uniques = list(set(train_set.labels))\n",
    "test_set_av.uniques = list(set(test_set.labels))\n",
    "val_set_av.uniques = list(set(val_set.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "LEhQaFGEQcTz"
   },
   "outputs": [],
   "source": [
    "bm = nc_benchmark(\n",
    "    train_dataset=train_set_av,\n",
    "    test_dataset=val_set_av,\n",
    "    n_experiences=11,\n",
    "    shuffle=False,\n",
    "    seed=4123,\n",
    "    task_labels=False,\n",
    "    class_ids_from_zero_in_each_exp=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE EVALUATION PLUGIN\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(experience=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    #confusion_matrix_metrics(num_classes=bm.n_classes, save_image=False, stream=True),\n",
    "    loggers=[InteractiveLogger()],\n",
    "    strict_checks=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN_224by224(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Dropout(p=0.25, inplace=False)\n",
      "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Dropout(p=0.25, inplace=False)\n",
      "    (12): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): AdaptiveMaxPool2d(output_size=1)\n",
      "    (15): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "interactive_logger = InteractiveLogger()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_batch_of_images(train_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_first_image(train_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_first_image(val_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_first_image(test_loader_av)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive strategy with cross entropy loss and adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yK88QPCmQzA_",
    "outputId": "88bc7c53-2fa7-4199-fd80-553531c49ced"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.01, weight_decay=1e-2)\n",
    "criterion = CrossEntropyLoss(label_smoothing=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive strategy\n",
    "cl_strategy = Naive(\n",
    "    model=model, optimizer=optimizer, criterion=criterion, train_mb_size=64, train_epochs=1, \n",
    "    eval_mb_size=16, device=device, evaluator=eval_plugin, \n",
    "    plugins=[ReplayPlugin(mem_size=2000), EarlyStoppingPlugin(patience=2, val_stream_name='valid')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "results = my_utils.cl_train_loop(bm, cl_strategy, model, optimizer, number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "taW1mJPKqLwz",
    "outputId": "8c8753d5-7fef-4dbe-9096-ab2b2b5039f4"
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "my_utils.checkpoint(model, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_Naive_CL_1_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VmpqErprNUq2",
    "outputId": "060f7b64-22b1-43ab-ce2a-34b089e89267"
   },
   "outputs": [],
   "source": [
    "my_utils.n_test_predictions(model, test_loader_av, classes, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nfxKV4NxtfAr",
    "outputId": "667232c3-a3c5-4601-e93b-ed9eebf43878"
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "y_pred_list, y_true_list = my_utils.get_predictions(model, test_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXDVLw9Mtt4v"
   },
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "id": "q4syrSY9t9N9",
    "outputId": "ab7af71c-e301-49c2-a0a8-0cdf621dd9f7"
   },
   "outputs": [],
   "source": [
    "my_utils.plot_confusion_matrix(cm, classes, 'cm_Naive_1_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbYSfxKDj3vj"
   },
   "outputs": [],
   "source": [
    "my_utils.plot_f1_scores(f1, classes, 'f1_scores_Naive_1_0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LwF strategy with cross entropy loss and adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN_224by224(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Dropout(p=0.25, inplace=False)\n",
      "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Dropout(p=0.25, inplace=False)\n",
      "    (12): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): AdaptiveMaxPool2d(output_size=1)\n",
      "    (15): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = my_architectures.SimpleCNN_224by224()\n",
    "model.to(device)\n",
    "interactive_logger = InteractiveLogger()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha and temperature are taken from the paper by Oren & Wolf - \"In defense of the Learning Without Forgetting for Task Incremental Learning\"\n",
    "cl_strategy = LwF(\n",
    "    model=model, optimizer=optimizer, criterion=criterion, alpha=0.25, temperature=2.0, \n",
    "    train_mb_size=64, train_epochs=2, eval_mb_size=32, device=device, evaluator=eval_plugin, \n",
    "    plugins=[ReplayPlugin(mem_size=2*len(train_set)] #Early stopping is not used here because LwF does not inherently support Early Stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1100"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment with strategy: <avalanche.training.supervised.strategy_wrappers.LwF object at 0x000002379AC71360>\n",
      "Start of experience:  0\n",
      "Current Classes:  [0, 1]\n",
      "2\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 2/2 [00:30<00:00, 15.36s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.6909\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.6991\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.5100\n",
      "100%|██████████| 2/2 [00:31<00:00, 15.61s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.6940\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.6979\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.5000\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Linear(in_features=64, out_features=4, bias=True)\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from train stream --\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.26s/it]\n",
      "> Eval on experience 0 (Task 0) from train stream ended.\n",
      "\tLoss_Exp/eval_phase/train_stream/Task000/Exp000 = 1.4076\n",
      "\tTime_Exp/eval_phase/train_stream/Task000/Exp000 = 5.0211\n",
      "\tTop1_Acc_Exp/eval_phase/train_stream/Task000/Exp000 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/train_stream/Task000 = 1.4076\n",
      "\tStreamForgetting/eval_phase/train_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/train_stream/Task000 = 0.0000\n",
      "Start of experience:  1\n",
      "Current Classes:  [2, 3]\n",
      "2\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 2/2 [00:52<00:00, 26.35s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.3874\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.3875\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2600\n",
      "100%|██████████| 2/2 [00:50<00:00, 25.16s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.3858\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.3857\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2350\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Linear(in_features=64, out_features=6, bias=True)\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 1 (Task 0) from train stream --\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.36s/it]\n",
      "> Eval on experience 1 (Task 0) from train stream ended.\n",
      "\tLoss_Exp/eval_phase/train_stream/Task000/Exp001 = 1.8047\n",
      "\tTime_Exp/eval_phase/train_stream/Task000/Exp001 = 5.4505\n",
      "\tTop1_Acc_Exp/eval_phase/train_stream/Task000/Exp001 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/train_stream/Task000 = 1.8047\n",
      "\tStreamForgetting/eval_phase/train_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/train_stream/Task000 = 0.0000\n",
      "Start of experience:  2\n",
      "Current Classes:  [4, 5]\n",
      "2\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 2/2 [00:56<00:00, 28.15s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.7781\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.7853\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2632\n",
      "100%|██████████| 2/2 [01:01<00:00, 30.52s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.7821\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.7949\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2105\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Linear(in_features=64, out_features=8, bias=True)\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 2 (Task 0) from train stream --\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.41s/it]\n",
      "> Eval on experience 2 (Task 0) from train stream ended.\n",
      "\tLoss_Exp/eval_phase/train_stream/Task000/Exp002 = 2.0103\n",
      "\tTime_Exp/eval_phase/train_stream/Task000/Exp002 = 5.6434\n",
      "\tTop1_Acc_Exp/eval_phase/train_stream/Task000/Exp002 = 0.5000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/train_stream/Task000 = 2.0103\n",
      "\tStreamForgetting/eval_phase/train_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/train_stream/Task000 = 0.5000\n",
      "Start of experience:  3\n",
      "Current Classes:  [6, 7]\n",
      "2\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 2/2 [00:55<00:00, 27.94s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.0755\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.0643\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1316\n",
      "100%|██████████| 2/2 [00:54<00:00, 27.48s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.0739\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.0696\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0877\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Linear(in_features=64, out_features=10, bias=True)\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 3 (Task 0) from train stream --\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.29s/it]\n",
      "> Eval on experience 3 (Task 0) from train stream ended.\n",
      "\tLoss_Exp/eval_phase/train_stream/Task000/Exp003 = 2.2945\n",
      "\tTime_Exp/eval_phase/train_stream/Task000/Exp003 = 5.1561\n",
      "\tTop1_Acc_Exp/eval_phase/train_stream/Task000/Exp003 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/train_stream/Task000 = 2.2945\n",
      "\tStreamForgetting/eval_phase/train_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/train_stream/Task000 = 0.0000\n",
      "Start of experience:  4\n",
      "Current Classes:  [8, 9]\n",
      "2\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 2/2 [00:59<00:00, 29.86s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2912\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2890\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0614\n",
      "100%|██████████| 2/2 [00:54<00:00, 27.27s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2912\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2927\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1316\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Linear(in_features=64, out_features=12, bias=True)\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 4 (Task 0) from train stream --\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.33s/it]\n",
      "> Eval on experience 4 (Task 0) from train stream ended.\n",
      "\tLoss_Exp/eval_phase/train_stream/Task000/Exp004 = 2.4648\n",
      "\tTime_Exp/eval_phase/train_stream/Task000/Exp004 = 5.3299\n",
      "\tTop1_Acc_Exp/eval_phase/train_stream/Task000/Exp004 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/train_stream/Task000 = 2.4648\n",
      "\tStreamForgetting/eval_phase/train_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/train_stream/Task000 = 0.0000\n",
      "Start of experience:  5\n",
      "Current Classes:  [10, 11]\n",
      "2\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 2/2 [00:55<00:00, 27.61s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4790\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.4802\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0965\n",
      "100%|██████████| 2/2 [00:56<00:00, 28.23s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4796\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.4795\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1184\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Linear(in_features=64, out_features=14, bias=True)\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 5 (Task 0) from train stream --\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.31s/it]\n",
      "> Eval on experience 5 (Task 0) from train stream ended.\n",
      "\tLoss_Exp/eval_phase/train_stream/Task000/Exp005 = 2.6201\n",
      "\tTime_Exp/eval_phase/train_stream/Task000/Exp005 = 5.2546\n",
      "\tTop1_Acc_Exp/eval_phase/train_stream/Task000/Exp005 = 0.5000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/train_stream/Task000 = 2.6201\n",
      "\tStreamForgetting/eval_phase/train_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/train_stream/Task000 = 0.5000\n",
      "Start of experience:  6\n",
      "Current Classes:  [12, 13]\n",
      "2\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 2/2 [01:01<00:00, 30.77s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.6434\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.6423\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0746\n",
      "100%|██████████| 2/2 [01:01<00:00, 30.68s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.6444\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.6447\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0482\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Linear(in_features=64, out_features=16, bias=True)\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 6 (Task 0) from train stream --\n",
      "100%|██████████| 4/4 [00:07<00:00,  1.76s/it]\n",
      "> Eval on experience 6 (Task 0) from train stream ended.\n",
      "\tLoss_Exp/eval_phase/train_stream/Task000/Exp006 = 2.7897\n",
      "\tTime_Exp/eval_phase/train_stream/Task000/Exp006 = 7.0458\n",
      "\tTop1_Acc_Exp/eval_phase/train_stream/Task000/Exp006 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/train_stream/Task000 = 2.7897\n",
      "\tStreamForgetting/eval_phase/train_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/train_stream/Task000 = 0.0000\n",
      "Start of experience:  7\n",
      "Current Classes:  [14, 15]\n",
      "2\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 2/2 [00:59<00:00, 29.50s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.7855\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.7897\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0219\n",
      "100%|██████████| 2/2 [01:02<00:00, 31.30s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.7803\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.7793\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0307\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Linear(in_features=64, out_features=18, bias=True)\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 7 (Task 0) from train stream --\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.39s/it]\n",
      "> Eval on experience 7 (Task 0) from train stream ended.\n",
      "\tLoss_Exp/eval_phase/train_stream/Task000/Exp007 = 2.8887\n",
      "\tTime_Exp/eval_phase/train_stream/Task000/Exp007 = 5.5711\n",
      "\tTop1_Acc_Exp/eval_phase/train_stream/Task000/Exp007 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/train_stream/Task000 = 2.8887\n",
      "\tStreamForgetting/eval_phase/train_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/train_stream/Task000 = 0.0000\n",
      "Start of experience:  8\n",
      "Current Classes:  [16, 17]\n",
      "2\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 2/2 [01:01<00:00, 30.63s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.9149\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.9096\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0439\n",
      "100%|██████████| 2/2 [00:55<00:00, 27.92s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.9167\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.9098\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0482\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Linear(in_features=64, out_features=20, bias=True)\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 8 (Task 0) from train stream --\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.39s/it]\n",
      "> Eval on experience 8 (Task 0) from train stream ended.\n",
      "\tLoss_Exp/eval_phase/train_stream/Task000/Exp008 = 3.0534\n",
      "\tTime_Exp/eval_phase/train_stream/Task000/Exp008 = 5.5457\n",
      "\tTop1_Acc_Exp/eval_phase/train_stream/Task000/Exp008 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/train_stream/Task000 = 3.0534\n",
      "\tStreamForgetting/eval_phase/train_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/train_stream/Task000 = 0.0000\n",
      "Start of experience:  9\n",
      "Current Classes:  [18, 19]\n",
      "2\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 2/2 [01:00<00:00, 30.43s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.9764\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.9771\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1140\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m results\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmy_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcl_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcl_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_workers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\MscThesis\\my_utils.py:167\u001b[0m, in \u001b[0;36mcl_train_loop\u001b[1;34m(bm, cl_strategy, model, optimizer, number_of_workers)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent Classes: \u001b[39m\u001b[38;5;124m\"\u001b[39m, experience\u001b[38;5;241m.\u001b[39mclasses_in_this_experience)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(experience\u001b[38;5;241m.\u001b[39mclasses_in_this_experience))\n\u001b[1;32m--> 167\u001b[0m cl_strategy\u001b[38;5;241m.\u001b[39mtrain(experience, num_workers\u001b[38;5;241m=\u001b[39mnumber_of_workers)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining completed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mclassifier\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m22\u001b[39m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\MscThesis\\lib\\site-packages\\avalanche\\training\\templates\\base_sgd.py:211\u001b[0m, in \u001b[0;36mBaseSGDTemplate.train\u001b[1;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    205\u001b[0m     experiences: Union[TDatasetExperience, Iterable[TDatasetExperience]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    210\u001b[0m ):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtrain(experiences, eval_streams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluator\u001b[38;5;241m.\u001b[39mget_last_metrics()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\MscThesis\\lib\\site-packages\\avalanche\\training\\templates\\base.py:163\u001b[0m, in \u001b[0;36mBaseTemplate.train\u001b[1;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperience \u001b[38;5;129;01min\u001b[39;00m experiences_list:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_training_exp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_exp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperience, eval_streams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_training_exp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_training(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\MscThesis\\lib\\site-packages\\avalanche\\training\\templates\\base_sgd.py:337\u001b[0m, in \u001b[0;36mBaseSGDTemplate._train_exp\u001b[1;34m(self, experience, eval_streams, **kwargs)\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_epoch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_training_epoch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\MscThesis\\lib\\site-packages\\avalanche\\training\\templates\\update_type\\sgd_update.py:19\u001b[0m, in \u001b[0;36mSGDUpdate.training_epoch\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Training epoch.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    :param kwargs:\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmbatch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader:\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_training:\n\u001b[0;32m     21\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\MscThesis\\lib\\site-packages\\avalanche\\benchmarks\\utils\\data_loader.py:199\u001b[0m, in \u001b[0;36mMultiDatasetDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_loader\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_loader()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\MscThesis\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:435\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\MscThesis\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:381\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\MscThesis\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1034\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1027\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1034\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\MscThesis\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\MscThesis\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\MscThesis\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\MscThesis\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\MscThesis\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "results = my_utils.cl_train_loop(bm, cl_strategy, model, optimizer, number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "my_utils.checkpoint(model, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_LwF_CL_1_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate predictions\n",
    "y_pred_list, y_true_list = my_utils.get_predictions(model, test_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_confusion_matrix(cm, classes, 'cm_LwF_1_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_f1_scores(f1, classes, 'f1_scores_LwF_1_0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGEM strategy with cross entropy loss and adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOrI+t9FJjnAgoquNJb9Lug",
   "include_colab_link": true,
   "mount_file_id": "1vsLt8KULa-1HhNdKYHdrNM4CC6hJY3lU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
