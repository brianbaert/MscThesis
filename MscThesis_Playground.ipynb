{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/brianbaert/MscThesis/blob/main/MscThesis_Playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOAiIcv_lMu8"
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LiPWZMxjETE3",
    "outputId": "7c1608f9-ddbd-48dd-d90c-9f886d526af0"
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P6lu0g2-XF2A",
    "outputId": "a86324e6-29d5-46fd-f27b-ee9980e3b025"
   },
   "outputs": [],
   "source": [
    "#!pip install avalanche_lib[all]\n",
    "#!pip install gwpy\n",
    "#!pip install nds2\n",
    "!pip show avalanche-lib\n",
    "!pip show gwpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t1jbz0_g5bZW",
    "outputId": "68b123d7-8ec2-4e8c-d4da-9c025017c0f4"
   },
   "outputs": [],
   "source": [
    " #%cd /content/drive/MyDrive/MscThesis\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rVEI5x8RadVA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import DatasetFolder, ImageFolder\n",
    "from tqdm import tqdm\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, \\\n",
    "accuracy_score\n",
    "from avalanche.models import SlimResNet18, MTSlimResNet18, SimpleCNN\n",
    "from avalanche.models import as_multitask, IncrementalClassifier\n",
    "from avalanche.models.utils import avalanche_model_adaptation\n",
    "from avalanche.training import Naive, LwF, GenerativeReplay, ICaRLLossPlugin, ICaRL, EWC, AR1, LFL\n",
    "from avalanche.logging import InteractiveLogger, TextLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin, ReplayPlugin, LwFPlugin, EarlyStoppingPlugin\n",
    "from avalanche.training.supervised import VAETraining\n",
    "from avalanche.benchmarks import nc_benchmark\n",
    "from avalanche.benchmarks.utils import AvalancheDataset\n",
    "from avalanche.benchmarks.scenarios.dataset_scenario import benchmark_from_datasets\n",
    "from avalanche.benchmarks.scenarios.supervised import class_incremental_benchmark\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics,loss_metrics, \\\n",
    "timing_metrics, cpu_usage_metrics, StreamConfusionMatrix,disk_usage_metrics, gpu_usage_metrics, \\\n",
    "confusion_matrix_metrics\n",
    "import multiprocessing as mp\n",
    "\n",
    "import my_utils\n",
    "import my_architectures\n",
    "import my_dataloaders\n",
    "import my_gwpy_and_fractals\n",
    "import my_transformations\n",
    "\n",
    "IMG_SIZE = (224,224)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "number_of_workers = mp.cpu_count()\n",
    "number_of_workers = int(number_of_workers/2)\n",
    "print(number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Wpa8snbd2P8Q"
   },
   "outputs": [],
   "source": [
    "train_dir2 = 'C:\\\\Users\\\\Brian.Baert\\\\TrainingSet'\n",
    "train_dir = 'C:\\\\Users\\\\Brian.Baert\\\\GravitySpy\\\\train\\\\train'\n",
    "val_dir = 'C:\\\\Users\\\\Brian.Baert\\\\GravitySpy\\\\validation\\\\validation'\n",
    "test_dir = 'C:\\\\Users\\\\Brian.Baert\\\\GravitySpy\\\\test\\\\test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jz23eVLldHhF",
    "outputId": "b43f99a3-4c6f-4eb2-d62a-8608422b899f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "Blip                   1274\n",
      "Koi_Fish                498\n",
      "Low_Frequency_Burst     437\n",
      "Light_Modulation        361\n",
      "Extremely_Loud          316\n",
      "Low_Frequency_Lines     315\n",
      "Power_Line              314\n",
      "Scattered_Light         308\n",
      "Violin_Mode             284\n",
      "Scratchy                237\n",
      "1080Lines               229\n",
      "Whistle                 208\n",
      "Helix                   195\n",
      "Repeating_Blips         185\n",
      "No_Glitch               107\n",
      "Tomte                    73\n",
      "1400Ripples              59\n",
      "None_of_the_Above        57\n",
      "Air_Compressor           41\n",
      "Chirp                    41\n",
      "Wandering_Line           29\n",
      "Paired_Doves             19\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "meta_train_v1 = pd.read_csv('C:\\\\Users\\\\Brian.Baert\\\\GravitySpy\\\\trainingset_v1d1_metadata.csv')\n",
    "print(meta_train_v1[meta_train_v1['sample_type']=='train']['label'].value_counts())\n",
    "classes = meta_train_v1['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kHz4V8FF7_AE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1080Lines': 0,\n",
       " '1400Ripples': 1,\n",
       " 'Air_Compressor': 2,\n",
       " 'Blip': 3,\n",
       " 'Chirp': 4,\n",
       " 'Extremely_Loud': 5,\n",
       " 'Helix': 6,\n",
       " 'Koi_Fish': 7,\n",
       " 'Light_Modulation': 8,\n",
       " 'Low_Frequency_Burst': 9,\n",
       " 'Low_Frequency_Lines': 10,\n",
       " 'None_of_the_Above': 11,\n",
       " 'No_Glitch': 12,\n",
       " 'Paired_Doves': 13,\n",
       " 'Power_Line': 14,\n",
       " 'Repeating_Blips': 15,\n",
       " 'Scattered_Light': 16,\n",
       " 'Scratchy': 17,\n",
       " 'Tomte': 18,\n",
       " 'Violin_Mode': 19,\n",
       " 'Wandering_Line': 20,\n",
       " 'Whistle': 21}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = my_utils.get_classes_from_dir(train_dir2)\n",
    "class_to_indx = my_utils.classes_to_indices(classes)\n",
    "class_to_indx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CW-cEM0DJMom",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# DL Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5luuVVS8cUE"
   },
   "outputs": [],
   "source": [
    "train_set = my_dataloaders.GravitySpy_1_0_dataset(root=train_dir, cls=classes, transform=transformGray)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "val_set = my_dataloaders.GravitySpy_1_0_dataset(root=val_dir, cls=classes, transform=transformGray)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=True, num_workers=number_of_workers)\n",
    "test_set = my_dataloaders.GravitySpy_1_0_dataset(root=test_dir, cls=classes, transform=transformGray)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOyw0KMTCZQ2"
   },
   "outputs": [],
   "source": [
    "train_set = my_dataloaders.GravitySpy_0_5_dataset(root=train_dir, cls=classes, transform=transformGray)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "val_set = my_dataloaders.GravitySpy_0_5_dataset(root=val_dir, cls=classes, transform=transformGray)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=True)\n",
    "test_set = my_dataloaders.GravitySpy_0_5_dataset(root=test_dir, cls=classes, transform=transformGray)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XuoIZcjFLCSk",
    "outputId": "41bcc824-052b-42a4-e8d6-acd450325da6"
   },
   "outputs": [],
   "source": [
    "print(\"The training loader contains {} instances, the val loader contains {} instances and the test loader contains {} instances\".format(len(train_loader), len(val_loader), len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgVNrbCl_OQo",
    "outputId": "fe5d2346-f865-4985-bf03-e766da533cfe"
   },
   "outputs": [],
   "source": [
    "class_counts = train_set.count_class_instances()\n",
    "class_weights = []\n",
    "for class_name, count in class_counts.items():\n",
    "  print(f\"{class_name}: {count} instances\")\n",
    "  class_weights.append(1.0/(count/len(train_set)))\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akFKILwW9FFq",
    "outputId": "d27e49d1-6889-40ae-f8b7-fe65dc640487"
   },
   "outputs": [],
   "source": [
    "# Create Neural Network architecture for finetuning\n",
    "myNet = my_architectures.BaselineGrayscaleNet_resnet18()\n",
    "myNet.to(device)\n",
    "print(myNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BP8GXkixkmAS"
   },
   "outputs": [],
   "source": [
    "# Pytorch Training loop\n",
    "epoch_test_loss = 0\n",
    "epoch_test_acc = 0\n",
    "epoch_test_correct = 0\n",
    "epoch_test_total = 0\n",
    "best_vloss = 1_000_000.\n",
    "avg_loss = 0\n",
    "last_loss = 0\n",
    "timestamp = my_utils.get_timestamp()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32), label_smoothing=0.2)\n",
    "optimizer = Adam(myNet.parameters(), lr=0.001, weight_decay=1e-3) #LR from the study of Tiago Fernandes\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 10\n",
    "epoch_number = 0\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "  total_correct = 0\n",
    "  total_samples = 0\n",
    "  print('EPOCH {}: '.format(epoch_number + 1))\n",
    "  train_loss = 0.0\n",
    "  myNet.train(True)\n",
    "\n",
    "  ## TRAINING ONE EPOCH\n",
    "  for i, data in enumerate(train_loader, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    # forward + backward + optimize\n",
    "    outputs = myNet(inputs)\n",
    "    # compute loss and gradient\n",
    "    loss = criterion(outputs, labels)\n",
    "    #add part of regularization\n",
    "    l2_reg = torch.tensor(0.0)\n",
    "    for param in myNet.parameters():\n",
    "      if param.requires_grad: #exclude frozen layers\n",
    "        l2_reg += torch.norm(param, p=2)\n",
    "    loss += 1e-5 * l2_reg\n",
    "    #end part of regularization\n",
    "    loss.backward()\n",
    "    # adjust learning weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    train_loss += loss.item()\n",
    "    total_correct += (predicted == labels).sum().item()\n",
    "    total_samples += labels.size(0)\n",
    "\n",
    "    if i % 16 == 15:\n",
    "      last_loss = train_loss / 16\n",
    "      print('  batch {} loss: {}, acc: {}'.format(i+1, last_loss, 100*total_correct/total_samples))\n",
    "      tb_x = epoch * len(train_loader) + i + 1\n",
    "      train_loss=0.\n",
    "    avg_loss = last_loss\n",
    "\n",
    "  ## VALIDATION\n",
    "  running_vloss = 0.0\n",
    "  myNet.eval()\n",
    "\n",
    "  # disable gradient computation for validation\n",
    "  with torch.no_grad():\n",
    "    for i, vdata in enumerate(val_loader):\n",
    "      vinputs, vlabels = vdata\n",
    "      vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
    "      voutputs = myNet(vinputs)\n",
    "      vloss = criterion(voutputs, vlabels)\n",
    "      running_vloss += vloss\n",
    "  avg_vloss = running_vloss / (i+1)\n",
    "  print('LOSS train {:.4f} valid {:.4f} after {} seconds'.format(avg_loss, avg_vloss, time.time()-start))\n",
    "\n",
    "  if avg_vloss < best_vloss:\n",
    "    best_vloss = avg_vloss\n",
    "    model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "    torch.save(myNet.state_dict(), model_path)\n",
    "\n",
    "  #my_utils.checkpoint(myNet, f\"epoch-{epoch}.pth\")\n",
    "  epoch_number += 1\n",
    "\n",
    "end = time.time()\n",
    "print('Training finished after ', end-start, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFwrV-TIlBCz",
    "outputId": "5ce8f0e2-ed19-4fc1-d36c-9708971b38d8"
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "#torch.save(myNet.state_dict(), '/content/drive/MyDrive/MscThesis/thesis_baseline_finetune_0_5.pth')\n",
    "torch.save(myNet.state_dict(), 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_baseline_finetune_0_5.pth')\n",
    "print(\"Saved Pytorch Model state to thesis_baseline.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPb3Hcs4RbsK"
   },
   "outputs": [],
   "source": [
    "#myNet.load_state_dict(torch.load('/content/drive/MyDrive/MscThesis/thesis_baseline_finetune_1_0.pth'))\n",
    "myNet.load_state_dict(torch.load('C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_baseline_finetune_1_0.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijMNYNHxBB8D",
    "outputId": "c266284b-0852-40de-b6b7-31660d998d47"
   },
   "outputs": [],
   "source": [
    "my_utils.n_test_predictions(myNet, test_loader, classes, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPqfIK6qK_mg"
   },
   "outputs": [],
   "source": [
    "y_pred_list, y_true_list = my_utils.get_predictions(myNet, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETUYsXlKM2CB"
   },
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "from sklearn.metrics import f1_score\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nt51bp1_lJa-"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "# Use seaborn heatmap for visualization\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=classes, yticklabels=classes)\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "figTemp = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "figTemp.savefig('confusion_matrix_baseline_1_0.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyT7cWcJQAhm"
   },
   "outputs": [],
   "source": [
    "print(f\"F1 Score for each class: {f1}\")\n",
    "print(f\"The average F1 score is: {f1_score(y_true_list, y_pred_list, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qt4Bt4mURPMY"
   },
   "outputs": [],
   "source": [
    "# Create a horizontal bar plot for F1 scores with different colors\n",
    "plt.figure(figsize=(10,7))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(classes)))\n",
    "plt.barh(np.arange(len(classes)), f1, color=colors, align='center', alpha=0.5)\n",
    "plt.yticks(np.arange(len(classes)), classes)\n",
    "plt.xlabel('F1 Score')\n",
    "plt.title('F1 Score for Each Class')\n",
    "figTemp = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "figTemp.savefig('f1_scores_baseline_1_0.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPZtEpHZHjCZ"
   },
   "source": [
    "# CL Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, dataloader, benchmark and eval plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JiVzZcsjHlWV"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = my_architectures.SimpleCNN_224by224(num_classes=22, initial_out_features=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "xaqHp2gOQb0j"
   },
   "outputs": [],
   "source": [
    "train_set = my_dataloaders.GravitySpy_1_0_dataset(root=train_dir2, cls=classes, transform=my_transformations.transformAV_224_Crop)\n",
    "val_set = my_dataloaders.GravitySpy_1_0_dataset(root=val_dir, cls=classes, transform=my_transformations.transformAV_224)\n",
    "test_set = my_dataloaders.GravitySpy_1_0_dataset(root=test_dir, cls=classes, transform=my_transformations.transformAV_224)\n",
    "\n",
    "train_set_av = AvalancheDataset(train_set)\n",
    "train_loader_av = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "\n",
    "val_set_av = AvalancheDataset(val_set)\n",
    "val_loader_av = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=True, num_workers=number_of_workers)\n",
    "\n",
    "test_set_av = AvalancheDataset(test_set)\n",
    "test_loader_av = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "\n",
    "train_set_av.targets = train_set.labels\n",
    "test_set_av.targets = test_set.labels\n",
    "val_set_av.targets = val_set.labels\n",
    "\n",
    "train_set_av.uniques = list(set(train_set.labels))\n",
    "test_set_av.uniques = list(set(test_set.labels))\n",
    "val_set_av.uniques = list(set(val_set.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LEhQaFGEQcTz"
   },
   "outputs": [],
   "source": [
    "bm = nc_benchmark(\n",
    "    train_dataset=train_set_av,\n",
    "    test_dataset=val_set_av,\n",
    "    n_experiences=11,\n",
    "    shuffle=False,\n",
    "    seed=4123,\n",
    "    task_labels=False,\n",
    "    class_ids_from_zero_in_each_exp=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE EVALUATION PLUGIN\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(experience=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    loggers=[InteractiveLogger()],\n",
    "    strict_checks=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN_224by224(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Dropout(p=0.25, inplace=False)\n",
      "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Dropout(p=0.25, inplace=False)\n",
      "    (12): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): AdaptiveMaxPool2d(output_size=1)\n",
      "    (15): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (classifier): Linear(in_features=64, out_features=22, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "interactive_logger = InteractiveLogger()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_first_image(train_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_first_image(val_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_first_image(test_loader_av)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive strategy with cross entropy loss and adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yK88QPCmQzA_",
    "outputId": "88bc7c53-2fa7-4199-fd80-553531c49ced"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.01, weight_decay=1e-2)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive strategy\n",
    "cl_strategy = Naive(\n",
    "    model=model, optimizer=optimizer, criterion=criterion, train_mb_size=32, train_epochs=2, \n",
    "    eval_mb_size=16, device=device, evaluator=eval_plugin, \n",
    "    plugins=[ReplayPlugin(mem_size=2*len(train_set)), EarlyStoppingPlugin(patience=2, val_stream_name='valid')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment with strategy: <avalanche.training.supervised.strategy_wrappers.Naive object at 0x000002DAFC84FB80>\n",
      "Start of experience:  0\n",
      "Current Classes:  [0, 1]\n",
      "2\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 4/4 [00:11<00:00,  2.80s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.9346\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.2280\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1600\n",
      "100%|██████████| 4/4 [00:11<00:00,  2.90s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.4602\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.5045\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.5300\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from train stream --\n",
      "100%|██████████| 7/7 [00:05<00:00,  1.29it/s]\n",
      "> Eval on experience 0 (Task 0) from train stream ended.\n",
      "\tLoss_Exp/eval_phase/train_stream/Task000/Exp000 = 1.1575\n",
      "\tTime_Exp/eval_phase/train_stream/Task000/Exp000 = 5.4207\n",
      "\tTop1_Acc_Exp/eval_phase/train_stream/Task000/Exp000 = 0.5000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/train_stream/Task000 = 1.1575\n",
      "\tStreamForgetting/eval_phase/train_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/train_stream/Task000 = 0.5000\n",
      "Start of experience:  1\n",
      "Current Classes:  [2, 3]\n",
      "2\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 4/4 [00:22<00:00,  5.70s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.5574\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.7321\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|██████████| 4/4 [00:25<00:00,  6.29s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.6437\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.5904\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2500\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 1 (Task 0) from train stream --\n",
      "100%|██████████| 7/7 [00:05<00:00,  1.31it/s]\n",
      "> Eval on experience 1 (Task 0) from train stream ended.\n",
      "\tLoss_Exp/eval_phase/train_stream/Task000/Exp001 = 2.6551\n",
      "\tTime_Exp/eval_phase/train_stream/Task000/Exp001 = 5.3404\n",
      "\tTop1_Acc_Exp/eval_phase/train_stream/Task000/Exp001 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/train_stream/Task000 = 2.6551\n",
      "\tStreamForgetting/eval_phase/train_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/train_stream/Task000 = 0.0000\n",
      "Start of experience:  2\n",
      "Current Classes:  [4, 5]\n",
      "2\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 4/4 [00:28<00:00,  7.19s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.6954\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1509\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1491\n",
      "100%|██████████| 4/4 [00:27<00:00,  7.00s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4167\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9411\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1447\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 2 (Task 0) from train stream --\n",
      "100%|██████████| 7/7 [00:05<00:00,  1.25it/s]\n",
      "> Eval on experience 2 (Task 0) from train stream ended.\n",
      "\tLoss_Exp/eval_phase/train_stream/Task000/Exp002 = 2.5666\n",
      "\tTime_Exp/eval_phase/train_stream/Task000/Exp002 = 5.6115\n",
      "\tTop1_Acc_Exp/eval_phase/train_stream/Task000/Exp002 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/train_stream/Task000 = 2.5666\n",
      "\tStreamForgetting/eval_phase/train_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/train_stream/Task000 = 0.0000\n",
      "Start of experience:  3\n",
      "Current Classes:  [6, 7]\n",
      "2\n",
      "-- >> Start of training phase << --\n",
      " 50%|█████     | 2/4 [00:15<00:15,  7.83s/it]"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "results = my_utils.cl_simple_train_loop(bm, cl_strategy, model, optimizer, number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "taW1mJPKqLwz",
    "outputId": "8c8753d5-7fef-4dbe-9096-ab2b2b5039f4"
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "my_utils.checkpoint(model, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_Naive_CL_1_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VmpqErprNUq2",
    "outputId": "060f7b64-22b1-43ab-ce2a-34b089e89267"
   },
   "outputs": [],
   "source": [
    "my_utils.n_test_predictions(model, test_loader_av, classes, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nfxKV4NxtfAr",
    "outputId": "667232c3-a3c5-4601-e93b-ed9eebf43878"
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "y_pred_list, y_true_list = my_utils.get_predictions(model, test_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXDVLw9Mtt4v"
   },
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "id": "q4syrSY9t9N9",
    "outputId": "ab7af71c-e301-49c2-a0a8-0cdf621dd9f7"
   },
   "outputs": [],
   "source": [
    "my_utils.plot_confusion_matrix(cm, classes, 'cm_Naive_1_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbYSfxKDj3vj"
   },
   "outputs": [],
   "source": [
    "my_utils.plot_f1_scores(f1, classes, 'f1_scores_Naive_1_0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LwF strategy with cross entropy loss and adam optimizer\n",
    "The experiments are done on a neural network starting with 2 output nodes and gradually adapting with each experience to 22, but also on a neural network with 22 output nodes and running over the experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_architectures.SimpleCNN_224by224(num_classes=22, initial_out_features=22)\n",
    "model.to(device)\n",
    "interactive_logger = InteractiveLogger()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.01, weight_decay=1e-2)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha and temperature are taken from the paper by Oren & Wolf - \"In defense of the Learning Without Forgetting for Task Incremental Learning\"\n",
    "cl_strategy = LwF(\n",
    "    model=model, optimizer=optimizer, criterion=criterion, alpha=0.25, temperature=2.0, \n",
    "    train_mb_size=64, train_epochs=2, eval_mb_size=32, device=device, evaluator=eval_plugin, \n",
    "    plugins=[ReplayPlugin(mem_size=2*len(train_set))] #Early stopping is not used here because LwF does not inherently support Early Stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "results = my_utils.cl_simple_train_loop(bm, cl_strategy, model, optimizer, number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "my_utils.checkpoint(model, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_LwF_CL_1_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate predictions\n",
    "y_pred_list, y_true_list = my_utils.get_predictions(model, test_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_confusion_matrix(cm, classes, 'cm_LwF_1_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_f1_scores(f1, classes, 'f1_scores_LwF_1_0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFL (Less Forgetful Learning) strategy with cross entropy loss and adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_architectures.MLP(initial_out_features=22)\n",
    "model.to(device)\n",
    "interactive_logger = InteractiveLogger()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(model.parameters(), lr=0.01)\n",
    "criterion = CrossEntropyLoss()\n",
    "lambda_e = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_strategy = LFL(\n",
    "    model=model, optimizer=optimizer, criterion=criterion, lambda_e=lambda_e, \n",
    "    train_mb_size=64, train_epochs=3, eval_mb_size=32, device=device, evaluator=eval_plugin\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "results = my_utils.cl_simple_train_loop(bm, cl_strategy, model, optimizer, number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "my_utils.checkpoint(model, 'C:\\\\Users\\\\Brian.Baert\\\\Documents\\\\GitHub\\\\MscThesis\\\\Models\\\\thesis_LFL_CL_1_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate predictions\n",
    "y_pred_list, y_true_list = my_utils.get_predictions(model, test_loader_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true_list, y_pred_list)\n",
    "f1 = f1_score(y_true_list, y_pred_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_confusion_matrix(cm, classes, 'cm_LFL_1_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utils.plot_f1_scores(f1, classes, 'f1_scores_LFL_1_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOrI+t9FJjnAgoquNJb9Lug",
   "include_colab_link": true,
   "mount_file_id": "1vsLt8KULa-1HhNdKYHdrNM4CC6hJY3lU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
